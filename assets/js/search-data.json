{
  
    
        "post0": {
            "title": "Neural network from scratch",
            "content": "To have a genuine understanding of neural networks is valuable because they feel like &quot;black boxes.&quot; To be sure we understand it, we should build one from scratch, so I decided to create one and write about it. In this post, I will build one from scratch for classifying MNIST dataset and touch on underlying concepts behind it. I hope it helps. . Prerequisities . Code is written in python 3.8+ and I use the library called NumPy for matrixes multiplication. The math behind neural networks can look daunting, but it is basically high school math. . This is everything I am going to import for building NN: . import numpy as np import matplotlib.pyplot as plt # for plotting arrays from tqdm import trange # for fancy loading console output . NumPy . Neural networks use tensor for passing multiple data through a network. In Python, we don&#39;t have tensors, but those can be represented with arrays from NumPy library. It provides n-dimensional arrays, faster computing, cleaner code, a few functions we&#39;re gonna use and it is commonly used for data science. If you&#39;re not familiar with NumPy, you can check this video to learn about it (or read about it). Numpy is there explained in details. Only one thing to Numpy: . Numpy arrays can be also plotted with its parent library called Matplotlib . a = np.eye(3, 3) plt.imshow(a) plt.show() . MNIST dataset . Before we start to build one, we have to import the dataset to work with. For that we need this code: . import requests, gzip from pathlib import Path # source: http://yann.lecun.com/exdb/mnist/ # modified from: https://github.com/geohot/ai-notebooks/blob/master/mnist_from_scratch.ipynb def fetch(url): name = url.split(&quot;/&quot;)[-1] dirs = Path(&quot;dataset/mnist&quot;) path = (dirs / name) if path.exists(): with path.open(&quot;rb&quot;) as f: dat = f.read() else: if not dirs.is_dir(): dirs.mkdir(exist_ok=True) with path.open(&quot;wb&quot;) as f: dat = requests.get(url).content f.write(dat) return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy() def mnist_dataset(): X_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;)[8:] X_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;)[8:] return (X_train, Y_train, X_test, Y_test) . MNIST dataset is made from 70000 images. It is split in two types: training (60k) &amp; testing (10k) data. Each image has 28 to 28 pixels and its own label, which describes it. . X_train, Y_train, X_test, Y_test = mnist_dataset() select = 1337 plt.imshow(X_train[select]) print(f&quot;Number: {Y_train[select]} | Shape: {X_train[select].shape}&quot;) . Number: 6 | Shape: (28, 28) . The reason we have a split dataset is for validating how (if even) our network functions properly. We will be passing images as flatten images through the neural network to &quot;show it&quot; the whole image. Right now we are going to pass only one image, but later we will be passing more at once as batch. So shape is: . select = 1337 plt.imshow(X_train[select].reshape(1, -1)) print(f&quot;Number {Y_train[select]} | Shape: {X_train[select].reshape(1, -1).shape}&quot;) . Number 6 | Shape: (1, 784) . . Note: We have that shape because it is only one image and 28 (pixels) multiply by 28 equal 784. . How it looks like? . To have some visual image of what we&#39;re going to build: . Source: https://github.com/fastai/fastbook/blob/master/01_intro.ipynb . . Important: Through out this blog post I am using termininology in this image. . This is how a Neural Network (NN) looks like! Our inputs are MNIST images in the form of a NumPy 2D array (matrix), which contains numbers between 0 to 1. Usually, we represent data between 0 to 1 or -1 to 1 (It is an unwritten golden rule). Architecture (model) is basically our neural network. There we pass inputs through nodes (neurons), which compute some operations with parameters and we get predictions. We then take labels (targets) and compare them with our prediction. We measure how wrong our model was with the loss function and use it to update parameters so it could &quot;learn&quot;. . Maybe now is a good question what is even neural network?. It is inspired by neurons in our brain. Neuron &quot;fire&quot; signal if it gets a certain amount of input and millions of them can create complex structures. For our case, we won&#39;t need millions of them, but only a few. Right now there are three types of structure in the field: linear, convolutional, and recurrent. We are going to use the simplest and oldest one the linear layer: . outputs = weights * inputs + biases . As we can see it is a linear function. In our model, we won&#39;t use biases, so weights are the only parameters we will be updating in the training loop. . . Note: With this, our model will be learning on its own and when our programming function is creating its own function, we call it machine learning. . So far, so good, but to clarify how our architecture will look like: every node (neuron) will be a linear function, and we stack them as a full layer and pass every input to every node, where each contains a parameter. We then stack multiple layers. . This image (below) roughly show how a neural network looks like. Here it is defined with: input layer (16 nodes), hidden layer 1 (12 nodes), hidden layer 2 (12 nodes), output layer (1 node). . Created with: https://alexlenail.me/NN-SVG/index.html . Each of those &quot;strings&quot; is weights (parameter) and we can see that every node has its own weight for every node from the previous layer. We will create only one hidden layer with 128 nodes. Our input layer is flattened, where each pixel is represented as one node in the input layer. The last layer is the output layer, and because we classify 0-9 images it will have 10 nodes. . . Note: When a neural network has more hidden layers we call it deep learning (which is a subset field of machine learning). . PyTorch example . To show you everything we will be implementing from scratch I have prepared an example made with a popular deep learning library called PyTorch. Just hit the &quot;show code&quot; button to have a picture of what in the code we are going to implement (from scratch). . #hide-output import torch import torch.nn as nn torch.manual_seed(7) class TorchNet(nn.Module): def __init__(self): super(TorchNet, self).__init__() self.l1 = nn.Linear(784, 128, bias=False) self.l2 = nn.Linear(128, 10) self.sm = nn.LogSoftmax(dim=1) self.act = nn.ReLU() def forward(self, x): x = self.act(self.l1(x)) x = self.l2(x) x = self.sm(x) return x model = TorchNet() loss_function = nn.NLLLoss(reduction=&#39;none&#39;) optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0) BS = 128 losses, accuracies = [], [] for i in (t := trange(10000)): # Create Batch (randomly initiated) samp = np.random.randint(0, X_train.shape[0], size=(BS)) X = torch.tensor(X_train[samp].reshape((-1, 28*28))).float() Y = torch.tensor(Y_train[samp]).long() # Pass through Net (+ calculate loss) output = model.forward(X) loss = loss_function(output, Y).mean() # Update parameters optim.zero_grad() loss.backward() optim.step() # Save for statistic cat = torch.argmax(output, dim=1) acc = (cat == Y).float().mean() accuracies.append(acc.item()) losses.append(loss.item()) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) . . It evaluated about 97% on the testing set. We will be recreating this whole code from scratch. I recommend to use this section as reference. . Neural network structure . As the first image show in the How it looks like?, we will be implementing everything that is there. The whole code for the neural network should be under 100 lines of code in Python. . Parameters (Weights) . We&#39;re going to have three layers: input layer (784 nodes), hidden layer (128 nodes), and output layer (10 nodes). We pass input to the first hidden layer and output will be pass forward to another layer to the next layer until it reaches the final layer (output layer). Each node will contain its own parameter, which will perform some operation with the input. We are going to use a simple linear layer (only weights), so our parameters will be only one array for each layer. . Every neural network library uses its own initiate functions that create usually numbers between -1 to 1 or 0 to 1. I chose uniform function because it performed best from what I have observed. . def _uniform(a, b, dtype=np.float32): return np.random.uniform(-1., 1., size=(a, b)).astype(dtype) / np.sqrt(a*b) . . Note: We&#8217;re going to use np.float32 for accuracy. All popular neural network libraries use them by default. . w1 = _uniform(784, 128) # layer one (weights) | represent input layer &amp; hidden layer 1 w2 = _uniform(128, 10) # layer two (weights) | represent hidden layer 1 &amp; output layer w1.shape . (784, 128) . In the code, weights (w1 and w2) are representing whole layer. We can see in _uniform(784, 128) and _uniform(128, 10) we input the whole all three layers of our model. . Architecture (Linear Layer &amp; Activation function) . We are representing linear layers as inputs multiply by weights. To perform our linear pass we take one image from the training set and use np.dot with our first parameters w1 and we compute the same with its output and second parameters w2. . We are representing linear layers as inputs multiply by weights. To perform our linear pass we take one image from the training set and use @ Python operator that multiply rows of first array with columns of second array. Alternatively in Numpy you can use np.dot(). . def linear(a, b): return a @ b # or np.dot(a, b) img = X_train[0].reshape(-1) x_l1 = linear(img, w1) x_l1.shape . (128,) . Just to clarify how linear function looks like: . With linear layers, we use an activation functions. They are non-linear function. We use them to have non-linear outputs. Imagine your input data are, for example, sinus. If our architecture would be only made by a linear function (linear layers), it couldn&#39;t learn how to &quot;bend&quot; output to something similar to the sinus. . . Note: Another reason we use them is the universal approximation theorem. Basically, with only two linear layers, we could rewrite it as one linear layer, but with a non-linearity (in our case between them) and sufficiently big matrices, it can approximate any arbitrary function. . We use it after the hidden layer. Because we only classify It isn&#39;t a necessity, but we still get better performance. We implement ReLU (Rectified linear unit), which is most used. ReLU is defined as if input &lt; 0 than 0 else input and to implement it is really easy: . def relu(x): return np.maximum(x, 0) . It looks like this: . Implementation with our code: . x_w1 = linear(X_train[0].reshape(1, -1), w1) x_relu = relu(x_w1) x_w2 = linear(x_relu, w2) x_w2 . array([[-1.2433078 , 1.5567372 , -0.33561662, 1.2811314 , -0.48807186, 0.0786325 , 0.3409022 , -0.66693485, 0.85523325, -0.47941723]], dtype=float32) . That&#39;s it. We have a forward pass for our neural network. After we pass the image array, we get an output of size 10. But the real question is How it will learn?. . Alternative activation function . Alternatively you can use second most used activation function called Sigmoid (keep in mind it will be more computationaly expensive). . def sigmoid(x): return 1/(1 + np.exp(-x)) . Loss function . To teach neural network we have to figure out how to represented how big is an error (difference) between prediction and label/target. . We than pass one of the images through our layers (Forward pass), that will give us array of shape (10,). With Numpy function np.argmax() we can extract that as single number by the position of the biggest number in the array. . select = 1337 Y = Y_train[select] # target / label X = X_train[select].reshape(1, -1) # input # Forward pass x_w1 = linear(X, w1) x_relu = relu(x_w1) x_w2 = linear(x_relu, w2) # output output = np.argmax(x_w2) # Take biggest number in array (by position) # Differences print(f&quot;Correct number: {Y}&quot;) print(f&quot;Prediction: {output}&quot;) . Correct number: 6 Prediction: 1 . We can see its prediction is wrong. We need to calculate error between output and label. Our output x_w2 is already array, so we need to transform label to array of same shape like x_w2, which is (10,). That can be done by using one-hot encoded array. . label_arr = np.zeros((10, ), np.float32) label_arr[Y] = 1 # Differences print(f&quot;Correct number: {label_arr} -&gt; {Y}&quot;) print(f&quot;Prediction: {x_w2} -&gt; {output}&quot;) . Correct number: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] -&gt; 6 Prediction: [[-0.460998 2.2878907 0.1116896 1.2516135 -0.2568604 0.18863007 0.19345318 -0.39150557 0.3726954 -0.5020878 ]] -&gt; 1 . Because we are classifying single digits, we have 10 categories. For that we will use Cross Entropy Loss. We want NN output to be distribution of how much sure it is, which can be represented as percentage between 0% to 100%. For that we need transform (squish) data of prediction between 0 to 1. Activation function which handle that is called Softmax (as graph it looks similiarly to Sigmoid, but code isn&#39;t). . def softmax(x): return np.exp(x) / np.exp(x).sum(axis=1) . . Note: Exponencial operation -&gt; np.exp(x) is defined as e ** x, where e is special (magical) number like pi. Its value is approximately equal to 2.718. . s_out = softmax(x_w2) s_out . array([[0.02999346, 0.4686556 , 0.05317919, 0.16626634, 0.03678599, 0.05743234, 0.05771001, 0.03215191, 0.06903909, 0.02878602]], dtype=float32) . The reason we want to use softmax is that if we sum it together, we get single digit (almost). . s_out.sum() . 0.99999994 . No matter how many categories we would have, sum of all of them will be always approximately equel to one. That is useful for our case (classifying -&gt; multi-category array) and measuring how wrong is our model. Another reason why use Softmax is that if our NN is confident about its prediction (one number is bigger than others). In Softmax that number will be magnified and far bigger than other. . Our second part of Cross entropy loss include Negative Log-Likelihood (NLL), which use logarithm: . def nll_loss(x): return np.log(x) . For clarification, here is difference between exponencial and logarithm: . It&#39;s not obvious, but logarithm is same as exponencial, but inverted. This will result in magnified values in output from Softmax, but our biggest number will become our smallest number in the array. We want to use NLL for making steeper learning curve in Backpropagation later. . loss = nll_loss(s_out) print(f&quot;Softmax: {s_out} nSum: {s_out.sum()} n&quot;) print(f&quot;NLL: {loss} nSum:: {loss.sum()}&quot;) . Softmax: [[0.02999346 0.4686556 0.05317919 0.16626634 0.03678599 0.05743234 0.05771001 0.03215191 0.06903909 0.02878602]] Sum: 0.9999999403953552 NLL: [[-3.5067759 -0.7578871 -2.9340882 -1.7941643 -3.3026383 -2.8571477 -2.8523247 -3.4372833 -2.6730824 -3.5478656]] Sum:: -27.663257598876953 . We just need to bundle it together and we get Cross Entropy Loss: . def crossentropyloss(x): return np.log(np.exp(x) / np.exp(x).sum(axis=1)) . Our last ingredient is to compare it against target in some meaningful way and that is done in Backpropagation section. . We have our loss function, but we won&#39;t be implementing it in the forward pass. We use the loss function to update our parameters, which are w1 and w2. Our loss function will be implemented in backward pass. . Stable code . As side note there is computationaly more stable way of the code of our loss function, which I will use. Its explanation you can find here. . def logsumexp(x): c = x.max(axis=1) return x - (c + np.log(np.exp(x-c.reshape((-1, 1))).sum(axis=1))).reshape((-1, 1)) . It seems like it is different, but it have same output: . loss1 = crossentropyloss(x_w2) loss2 = logsumexp(x_w2) loss1.sum(), loss2.sum() . (-27.663258, -27.663258) . Backpropagation . The fundamental question is How do you meaningfully update parameters?. The answer is we use derivatives of functions we implemented in architecture (model). Derivatives are representing the slope of the original function. All those results from each layer give us a multidimensional representation called gradient descent. We calculate them to find how much we need to increase or decrease parameters so we would decrease a loss and update parameters. This part is conceptually the hardest one to understand. . Let&#39;s say we have only one small linear hidden layer with a loss function and we get this: . Our goal is to update parameters to decrease a loss, which represents how the wrong model was so it would be next time more accurate. If we calculate derivative we can get something like in the graph above (probably with more dimensions). Let&#39;s take that red dot as one of the weights (parameters) and calculate the derivative of loss with respect to the parameters and we get something that can be represented like this: . . Important: It is not obvious, but by moving the parameters, we cause to some of them to be less important than the others, and with different inputs, each parameter will have different impact on the result. . Now we know the slope, it just needs some push to the right, and for that, we will use learning rate and optimizer later. . We are interested only in the three types of derivatives function. Those are Linear, ReLU, and LogSoftmax. Also, we will need for LogSoftmax a derivative one-hot encoded label (target), so how do we even compute derivatives? . Remember that derivatives are just slopes. If we take, for example, a linear function like this one: . def linear_function(x, a=-2, b=-1): return a * x + b . To get slope we take some two coordinations on the line like (x=-2, y=2.5) and (x=-4, y=5) and calculate it: . slope = (y2 - y1) / (x2 - x1) slope = (-5 - 2.5) / (-4 -2) slope = -1.25 . Result is -1.25; that means if we take some x coordination like, for example, x2 and multiply it with the slope, we get y2 coordination. We then use that slope to move around its gradient descent (the first graph in this section) to find a global minimum (lowest point), but we will have far more complex functions to derivate, and to be accurate, we pick as second coordinate something that is closest to first picked coordinations. . This isn&#39;t much elegant solution, and because of that, neural network libraries use Leibniz Notations because they calculate slope instantly (without second coordinates), which is far accurate and faster. To demonstrate it: . y = 2x y&#39; = d/dx * 2x y&#39; = d/d * 2 y&#39; = 2 . You can read more about it here here or I recommend Khan Academy. . Now is time to implement it for our functions. You can checkout all activation functions and their derivatives here (checkout there sigmoid function and its derivative function to spot their differencies and remember that derivative function is a slope of original funciton). . Some derivative need to remember input from the forward pass, and because we start with a loss (from loss function), which is &quot;at the end of the forward pass,&quot; we need to compute backward, which we will compute it as a chain that put the result of the previous derivative to another (those are called gradients). . If you want to know in detail the derivative of Softmax &amp; NLL. For linear (w1 and w2), it is just the input from the original function multiplied by the previous gradiant output. . For ReLU it is: . f(x) = 0 if x&#39; &lt; 0 x if x&#39; &gt;= 0 . where x is the gradient (output from the previous derivation) and x&#39; is input from w1 | . We will also need to use .T Numpy function (transpose) that swap rows and columns, so we would have correct shape for matrix multiplication. . Luckily implementation in the code of backpropagation is fairly easy: . X = X_train[1337].reshape(1, 784) Y = np.array(Y_train[1337]).reshape(-1) # &lt;- target ## backward pass # target -&gt; one-hot encoded out = np.zeros((len(Y), 10), np.float32) out[range(out.shape[0]), Y] = 1 # crossentropyloss x_lsm = logsumexp(x_w2) # Chain of derivates # derivative of target d_out = -out / len(Y) # derivative of loss with respect to target dx_lsm = d_out - np.exp(x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 d_w2 = x_relu.T @ dx_lsm # derivative of ReLU d_relu = dx_lsm @ w2.T d_relu[x_w1 &lt; 0] = 0 # derivative of l1 d_w1 = X.T @ d_relu . There is only one last missing piece. We need to update parameters and for that, we use optimizer. . . Note: If you are still struggling with this part, then I recommend this video series from 3blue1brown, where gradient descent and backpropagation is visualized. You can compare it side by side with the code written so far. . Optimizer . We will use a simple one called SGD (Stochastic gradient descent). We will need a learning rate to say derivatives of parameters how big step we want to do in the gradient descent and update parameters. If our learning rate is too big, it will not reach global minimum (it will over shoot it). . If our learning rate is too small, it will get stuck at a local minimum (obviously we want global minimum). . Source: https://www.bdhammel.com/learning-rates/ . Forunet . # learning rate lr = 0.001 # SGD w1 += -lr * d_w1 w2 += -lr * d_w2 . . Note: The reasons for the learning rate to be that &quot;small&quot; is that our weights are initiated between -1 to 1. . That&#39;s all! We just need to construct a training loop and also batches of data. . Prepare data in a batches . So far we have been reshaping data with the shape (1, 784), but we will be computing multiple images at once for better results to achieve better generalizing. We initiate batch randomly. . batch_size = 128 samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] . Commonly is used batch sizes of 64, 128, 256. We have to be careful with batch sizes and learning rate; else we could overfit the model. That means our model would &quot;memorize&quot; and not generalize (optimal), which would result in worse performance on new unseen data (you can try it by initiating batches orderly). . Source: https://www.fastaireference.com/overfitting . Constructing training loop . When we get output from our model we treat it as one-hot encoded array and we transform it to a single number than we compare it with labels (targets) to keep track of accuracy. Also, we will track loss (which we need to sum together -&gt; to get single value). After the training loop, we will plot losses and accuracies. . output = x_w2 cat = np.argmax(output, axis=1) # from (10,) to (1) acc = (cat == Y).mean() x_loss = (-out * x_lsm).mean(axis=1) loss = x_loss.mean() loss, acc, cat . (0.28523248, 0.0859375, array([1], dtype=int64)) . Let&#39;s construct everything together and run it: . w1 = _uniform(784, 128) w2 = _uniform(128, 10) lr = 0.001 batch_size = 128 losses, accuracies = [], [] ## Train for i in (t := trange(10000)): # Batch of training data &amp; target data samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] ## Forward pass x_w1 = linear(X, w1) x_relu = relu(x_w1) x_w2 = linear(x_relu, w2) output = x_w2 ## backward pass # target -&gt; one-hot encoded out = np.zeros((len(Y),10), np.float32) out[range(out.shape[0]), Y] = 1 # crossentropyloss x_lsm = logsumexp(x_w2) # loss x_loss = (-out * x_lsm).mean(axis=1) loss = x_loss.mean() # derivative of target d_out = -out / len(Y) # derivative of loss with respect to target dx_lsm = d_out - np.exp(x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 d_w2 = x_relu.T @ dx_lsm # derivative of ReLU d_relu = dx_lsm @ w2.T d_relu[x_w1 &lt; 0] = 0 # derivative of l1 d_w1 = X.T @ d_relu ## Update # SGD w1 += -lr * d_w1 w2 += -lr * d_w2 # Save for statistic cat = np.argmax(output, axis=1) acc = (cat == Y).mean() accuracies.append(acc) losses.append(loss) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) plt.ylim(-0.1, 1.1) plt.plot(losses) plt.plot(accuracies) plt.legend([&quot;losses&quot;, &quot;accuracies&quot;]) plt.show() . Loss: 0.00129; Acc: 1.00000: 100%|██████████| 10000/10000 [00:43&lt;00:00, 228.66it/s] . It looks good, but we need to evaluate it. We do it by inputting the whole testing set. We don&#39;t need the backward pass anymore if we don&#39;t plan to update it. . # Forward pass x_w1 = linear(X_test.reshape((-1, 28*28)), w1) x_relu = relu(x_w1) output = linear(x_relu, w2) # Measure accuracy Y_test_preds = np.argmax(output, axis=1) true_acc = (Y_test == Y_test_preds).mean() print(f&quot;Accuracy on testing set: {true_acc}&quot;) . Accuracy on testing set: 0.9809 . We have successfully build a neural network from scratch that evaluates around the human level. Here is the whole code: (I have edited little the code to resemble PyTorch example): . #hide-output import numpy as np import matplotlib.pyplot as plt from tqdm import trange import requests, gzip from pathlib import Path def fetch(url): name = url.split(&quot;/&quot;)[-1] dirs = Path(&quot;dataset/mnist&quot;) path = (dirs / name) if path.exists(): with path.open(&quot;rb&quot;) as f: dat = f.read() else: if not dirs.is_dir(): dirs.mkdir(exist_ok=True) with path.open(&quot;wb&quot;) as f: dat = requests.get(url).content f.write(dat) return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy() def mnist_dataset(): X_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;)[8:] X_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;)[8:] return (X_train, Y_train, X_test, Y_test) X_train, Y_train, X_test, Y_test = mnist_dataset() def _uniform(a, b, dtype=np.float32): return np.random.uniform(-1., 1., size=(a, b)).astype(dtype) / np.sqrt(a*b) def linear(a, b): return a @ b # np.dot(a, b) def relu(x): return np.maximum(x, 0) def logsumexp(x): c = x.max(axis=1) return x - (c + np.log(np.exp(x-c.reshape((-1, 1))).sum(axis=1))).reshape((-1, 1)) class Net: def __init__(self, lr=0.001): # parameters self.w1 = _uniform(784, 128) self.w2 = _uniform(128, 10) self.lr = lr def calc_loss(self): # crossentropyloss self.x_lsm = logsumexp(self.x_w2) self.loss = (-self.out * self.x_lsm).mean(axis=1).mean() def calc_optim(self): # Update (SGD) self.w1 += -self.lr * self.d_w1 self.w2 += -self.lr * self.d_w2 def forward(self, X): self.X = X self.x_w1 = linear(X, self.w1) self.x_relu = relu(self.x_w1) self.x_w2 = linear(self.x_relu, self.w2) return self.x_w2 def backward(self, Y): # target -&gt; one-hot encoded self.out = np.zeros((len(Y),10), np.float32) self.out[range(self.out.shape[0]), Y] = 1 # loss function self.calc_loss() # derivative of target d_out = -self.out / len(Y) # derivative of loss with respect to target dx_lsm = d_out - np.exp(self.x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 self.d_w2 = self.x_relu.T @ dx_lsm # derivative of ReLU d_relu = dx_lsm @ self.w2.T d_relu[self.x_w1 &lt; 0] = 0 # derivative of l1 self.d_w1 = self.X.T @ d_relu net = Net() batch_size = 128 losses, accuracies = [], [] ## Train for i in (t := trange(10000)): # Batch of training data &amp; target data samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] output = net.forward(X) net.backward(Y) net.calc_optim() # Save for statistic cat = np.argmax(output, axis=1) # results from Net acc = (cat == Y).mean() accuracies.append(acc) losses.append(net.loss) t.set_description(f&quot;Loss: {net.loss:.5f}; Acc: {acc:.5f}&quot;) plt.ylim(-0.01, 1.1) plt.plot(losses) plt.plot(accuracies) plt.legend([&quot;losses&quot;, &quot;accuracies&quot;]) plt.show() ## Evaluation Y_test_preds = np.argmax(net.forward(X_test.reshape((-1, 28*28))), axis=1) true_acc = (Y_test == Y_test_preds).mean() print(f&quot;Accuracy on testing set: {true_acc}&quot;) . . You can compare it with the code from the PyTorch example and see differences. . Try it (Binder) . If you want try NN right now, I have prepared here widgets for uploading and passing Image to NN. Those widgets won&#39;t show up here, so you need to click on Binder button at the beginning of this blog post. It will generate you web version of jupyter notebook of this blog post. There run every cell and go to this section and here you will see widgets for uploading and evaluating image (image will be automatically rescale to 28x28 and convert to grayscale): . from ipywidgets import VBox, FileUpload, Button, Output, Label from IPython.display import display from PIL import Image, ImageOps import io btn_upload = FileUpload() btn_run = Button(description=&quot;Classify&quot;) out_pl = Output() lbl_pred = Label() def on_click_classify(change): # Convert Image from bytes to PIL format and than to numpy array fn = io.BytesIO(btn_upload.data[-1]) img = Image.open(fn).resize((28, 28)) img_ = ImageOps.grayscale(img) arr = np.asarray(img_) # get values from NN and show them output = net.forward(arr.reshape((1, 28*28))) predict = np.argmax(output, axis=1) confid = softmax(output) # display out_pl.clear_output() with out_pl: display(img_) lbl_pred.value = f&quot;Prediction: {predict[0]}; Confidence: {confid[0][predict[0]]:.4f}&quot; btn_run.on_click(on_click_classify) VBox([Label(&quot;Select your digit! (will classify last uploaded)&quot;), btn_upload, btn_run, out_pl, lbl_pred]) . Conclusion . I hope it helped. We have made a neural network that classifies digits on a human level. When I was starting with neural networks, I was often confused with the concepts behind them and their implementations in the code, but the best way to learn is by doing and keep pushing. . If you would like to continue, I recommend to continue with Pytorch library. Instead of arrays it use tensors, which are similiar to Numpy arrays, but tensor can run on GPU, which is great for big computations, it is build for deep learning and it is most used option library for publication of white papers. . If you would like some resources to learn Pytorch and NN, I recommend these resources (from which I have learned): . https://www.fast.ai | https://www.amazon.com/gp/product/B01EER4Z4G/ref=ppx_yo_dt_b_d_asin_title_o03?ie=UTF8&amp;psc=1 (beginner as coder) | https://www.amazon.com/gp/product/B085Z96M9P/ref=dbs_a_def_rwt_bibl_vppi_i1 | .",
            "url": "https://ludius0.github.io/my-blog/ai/deep%20learning%20(dl)/2020/12/14/Neural-network-from-scratch.html",
            "relUrl": "/ai/deep%20learning%20(dl)/2020/12/14/Neural-network-from-scratch.html",
            "date": " • Dec 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m ludius0. I have been programming almost year and I have been finishing my last high school year. I don’t have social media, but I have github account and youtube channel. . Also I generated this website with fastpages 1. . a blogging platform that natively supports Jupyter notebooks. It’s great check it out if you want also do blogging. &#8617; . |",
          "url": "https://ludius0.github.io/my-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ludius0.github.io/my-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}