{
  
    
        "post0": {
            "title": "Neural network from scratch",
            "content": "To have a genuine understanding of neural networks is valuable because they feel like &quot;black boxes.&quot; To be sure we understand it, we should build one from scratch, so I decided to create one and write about it. In this post, I will build one from scratch for classifying MNIST dataset and touch on underlying concepts behind it. I hope it helps. . Prerequisities . Code is written in python 3.8+ and I use the library called NumPy for matrixes multiplication. The math behind neural networks can look daunting, but it is basically high school math. . This is everything I am going to import for building NN: . import numpy as np import matplotlib.pyplot as plt # for plotting arrays from tqdm import trange # for fancy loading console output . NumPy . Neural networks use tensor for passing multiple data through a network. In python, we don&#39;t have tensors, but those can be represented with arrays from NumPy library. It provides n-dimensional arrays, faster computing, cleaner code, a few functions we&#39;re gonna use and it is commonly used for data science. If you&#39;re not familiar with NumPy, you can check this video to learn about it. Numpy is there explained in details, but only two points to NumPy: . NumPy arrays can be also plotted with its parent library called Matplotlib . a = np.eye(3, 3) plt.imshow(a) plt.show() . and we&#39;ll use a special operation called np.dot, which multiplies two arrays by taking rows of the first array and columns of the second array. If our array doesn&#39;t match the needed shape we can flip it with .T operation (transpose). . arr1 = np.random.uniform(-1, 1, (2, 4)) arr2 = np.random.uniform(-1, 1, (2, 4)) np.dot(arr1.T, arr2) . array([[-0.5081598 , 0.29138089, 0.67837477, 0.19979245], [-0.27515243, 0.77095284, -0.42650189, 1.28282271], [-0.49381512, 0.70157455, 0.11754107, 0.9956999 ], [ 0.52683367, -0.33922572, -0.65522606, -0.27827649]]) . Alternatively, you can use the Python @ operator for matrix multiplications. . arr1.T @ arr2 . array([[-0.5081598 , 0.29138089, 0.67837477, 0.19979245], [-0.27515243, 0.77095284, -0.42650189, 1.28282271], [-0.49381512, 0.70157455, 0.11754107, 0.9956999 ], [ 0.52683367, -0.33922572, -0.65522606, -0.27827649]]) . MNIST dataset . Before we start to build one, we have to import the dataset to work with. For that we need this code: . import requests, gzip, os, hashlib # source: http://yann.lecun.com/exdb/mnist/ # modified from: https://github.com/geohot/ai-notebooks/blob/master/mnist_from_scratch.ipynb def fetch(url): filename = url.split(&quot;/&quot;)[-1] full_filename = &quot;dataset/mnist/&quot; + f&quot;{filename}&quot; if os.path.isfile(full_filename): with open(full_filename, &quot;rb&quot;) as f: dat = f.read() else: if not os.path.isdir(&quot;dataset&quot;): os.mkdir(&quot;dataset&quot;) if not os.path.isdir(&quot;dataset/mnist&quot;): os.mkdir(&quot;dataset/mnist&quot;) with open(full_filename, &quot;wb&quot;) as f: dat = requests.get(url).content f.write(dat) return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy() def mnist_dataset(): X_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;)[8:] X_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;)[8:] return (X_train, Y_train, X_test, Y_test) . MNIST dataset is made from 70000 images. It is split in two types: training (60k) &amp; testing (10k) data. Each image has 28 to 28 pixels and its own label, which describes it. . X_train, Y_train, X_test, Y_test = mnist_dataset() select = 1337 plt.imshow(X_train[select]) print(f&quot;Number: {Y_train[select]} | Shape: {X_train[select].shape}&quot;) . Number: 6 | Shape: (28, 28) . The reason we have a split dataset is for validating how (if even) our network functions properly. We will be passing images as flatten images through the neural network to &quot;show it&quot; the whole image. Right now we are going to pass only one image, but later we will be passing more at once as batch. So shape is: . select = 1337 plt.imshow(X_train[select].reshape(1, -1)) print(f&quot;Number {Y_train[select]} | Shape: {X_train[select].reshape(1, -1).shape}&quot;) . Number 6 | Shape: (1, 784) . . Note: We have that shape because it is only one image and 28 (pixels) multiply by 28 equal 784. . How it looks like? . To have some visual image of what we&#39;re going to build: . Credit: https://github.com/fastai/fastbook/blob/master/01_intro.ipynb . . Important: Through out this blog post I am using termininology in this image. . This is how a Neural Network (NN) looks like! Our inputs are MNIST images in the form of a NumPy 2D array (matrix), which contains numbers between 0 to 1. Usually, we represent data between 0 to 1 or -1 to 1 (It is an unwritten golden rule). Architecture (model) is basically our neural network. There we pass inputs through nodes (neurons), which compute some operations with parameters and we get predictions. We then take labels (targets) and compare them with our prediction. We measure how wrong our model was with the loss function and use it to update parameters so it could &quot;learn&quot;. . Maybe now is a good question what is even neural network?. It is inspired by neurons in our brain. Neuron &quot;fire&quot; signal if it gets a certain amount of input and millions of them can create complex structures. For our case, we won&#39;t need millions of them, but only a few. Right now there are three types of structure in the field: linear, convolutional, and recurrent. We are going to use the simplest and oldest one the linear layer: . outputs = weights * inputs + biases . As we can see it is a linear function. In our model, we won&#39;t use biases, so weights are the only parameters we will be updating in the training loop. . . Note: With this, our model will be learning on its own and when our programming function is creating its own function, we call it machine learning. . So far, so good, but to clarify how our architecture will look like: every node (neuron) will be a linear function, and we stack them as a full layer and pass every input to every node, where each contains a parameter. We then stack multiple layers. . This image (below) roughly show how a neural network looks like. Here it is defined with: input layer (16 nodes), hidden layer 1 (12 nodes), hidden layer 2 (12 nodes), output layer (1 node). . Created with: https://alexlenail.me/NN-SVG/index.html . Each of those &quot;strings&quot; is weights (parameter) and we can see that every node has its own weight for every node from the previous layer. We will create only one hidden layer with 128 nodes. Our input layer is flattened, where each pixel is represented as one node in the input layer. The last layer is the output layer, and because we classify 0-9 images it will have 10 nodes. . . Note: When a neural network has more hidden layers we call it deep learning (which is a subset field of machine learning). . PyTorch example . To show you everything we will be implementing from scratch I have prepared an example made with a popular deep learning library called PyTorch. Just hit the &quot;show code&quot; button to have a picture of what in the code we are going to implement (from scratch). . #hide-output import torch import torch.nn as nn torch.manual_seed(7) class TorchNet(nn.Module): def __init__(self): super(TorchNet, self).__init__() self.l1 = nn.Linear(784, 128, bias=False) self.l2 = nn.Linear(128, 10) self.sm = nn.LogSoftmax(dim=1) self.act = nn.ReLU() def forward(self, x): x = self.act(self.l1(x)) x = self.l2(x) x = self.sm(x) return x model = TorchNet() loss_function = nn.NLLLoss(reduction=&#39;none&#39;) optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0) BS = 128 losses, accuracies = [], [] for i in (t := trange(10000)): # Create Batch (randomly initiated) samp = np.random.randint(0, X_train.shape[0], size=(BS)) X = torch.tensor(X_train[samp].reshape((-1, 28*28))).float() Y = torch.tensor(Y_train[samp]).long() # Pass through Net (+ calculate loss) output = model.forward(X) loss = loss_function(output, Y).mean() # Update parameters optim.zero_grad() loss.backward() optim.step() # Save for statistic cat = torch.argmax(output, dim=1) acc = (cat == Y).float().mean() accuracies.append(acc.item()) losses.append(loss.item()) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) . . It evaluated about 96% on the testing set. We will be recreating this whole code from scratch. . Neural network structure . As the first image show in the How it looks like?, we will be implementing everything that is there. The whole code for the neural network should be under 100 lines of code in python. . Parameters (Weights) . We&#39;re going to have three layers: input layer (784 nodes), hidden layer (128 nodes), and output layer (10 nodes). We pass input to the first hidden layer and output will be pass forward to another layer to the next layer until it reaches the final layer (output layer). Each node will contain its own parameter, which will perform some operation with the input. We are going to use a simple linear layer (only weights), so our parameters will be only one array for each layer. . Every neural network library uses its own initiate functions that create usually numbers between -1 to 1 or 0 to 1. I chose uniform function because it performed best from what I have observed. . def _uniform(a, b, dtype=np.float32): return np.random.uniform(-1., 1., size=(a, b)).astype(dtype) / np.sqrt(a*b) . . Note: We&#8217;re going to use np.float32 for accuracy. All popular neural network libraries use them by default. . w1 = _uniform(784, 128) # layer one (weights) | represent input layer &amp; hidden layer 1 w2 = _uniform(128, 10) # layer two (weights) | represent hidden layer 1 &amp; output layer w1.shape . (784, 128) . In the code, weights (w1 and w2) are representing whole layer. We can see in _uniform(784, 128) and _uniform(128, 10) we input the whole all three layers of our model. . Architecture (Linear Layer &amp; Activation function) . We are representing linear layers as inputs multiply by weights. To perform our linear pass we take one image from the training set and use np.dot with our first parameters w1 and we compute the same with its output and second parameters w2. . def linear(a, b): return np.dot(a, b) img = X_train[0].reshape(-1) x_l1 = linear(img, w1) x_l1.shape . (128,) . Just to clarify how linear function looks like: . With linear layers, we use an activation functions. They are non-linear function. We use them to have non-linear outputs. Imagine your input data are, for example, sinus. If our architecture would be only made by a linear function (linear layers), it couldn&#39;t learn how to &quot;bend&quot; output to something similar to the sinus. . We use it after the hidden layer. Because we only classify It isn&#39;t a necessity, but we still get better performance. We implement ReLU (Rectified linear unit), which is most used. ReLU is defined as if input &lt; 0 than 0 else input and to implement it is really easy: . def relu(x): return np.maximum(x, 0) . It looks like this: . Implementation with our code: . x_w1 = linear(X_train[0].reshape(1, -1), w1) x_relu = relu(x_w1) x_w2 = linear(x_relu, w2) x_w2 . array([[ 0.541123 , -0.5889548 , -0.08830152, -0.75051427, -0.9467809 , 0.6630673 , 0.544203 , 0.57855046, 0.09019433, 0.0677426 ]], dtype=float32) . That&#39;s it. We have a forward pass for our neural network. After we pass the image array, we get an output of size 10. But the real question is How it will learn?. . Loss function . To teach neural network we have to figure out how to represented how big is an error (difference) between prediction and label. . We can represent the label as one-hot encoded array, so if it&#39;s number 6 it creates an array of zeros (10,) and places number one on its 6 positions. . select = 1337 Y = Y_train[select] # target / label X = X_train[select].reshape(1, -1) # input # Represent label as one-hot encoded array label_arr = np.zeros((10, ), np.float32) label_arr[Y] = 1 # Forward pass x_w1 = np.dot(X, w1) x_relu = relu(x_w1) x_w2 = np.dot(x_relu, w2) # output output = np.argmax(x_w2) # Take biggest number in array (by position) # Differences print(f&quot;Correct number: {label_arr} -&gt; {Y}&quot;) print(f&quot;Prediction: {x_w2} -&gt; {output}&quot;) . Correct number: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] -&gt; 6 Prediction: [[-0.85878235 -0.665328 -0.11946356 -1.221584 -0.23490652 1.068969 1.7923663 1.8005888 0.30687174 0.8864175 ]] -&gt; 7 . We can see its prediction is wrong. We can calculate error simply like target - output, but because our classes (targets/labels) are mutually exclusive, we will use the specialized function for classification called LogSoftmax. With that, our differences are &quot;magnified.&quot; Here is the implementation in a code: . def logsoftmax(x): return x - np.log(np.exp(x).sum(axis=1)).reshape((-1, 1)) x_lsm = logsoftmax(x_w2) x_lsm . array([[-3.934499 , -3.7410448, -3.1951802, -4.297301 , -3.3106232, -2.0067477, -1.2833505, -1.2751279, -2.768845 , -2.189299 ]], dtype=float32) . We have our loss function, but we won&#39;t be implementing it in the forward pass. We use the loss function to update our parameters, which are w1 and w2, we will implement the backward pass. . Backpropagation . The fundamental question is How do you meaningfully update parameters?. The answer is we use derivatives of functions we implemented in architecture (model). Derivatives are representing the slope of the original function. All those results from each layer give us a multidimensional representation called gradient descent. We calculate them to find how much we need to increase or decrease parameters so we would decrease a loss and update parameters. This part is conceptually the hardest one to understand. . Let&#39;s say we have only one small linear hidden layer with a loss function and we get this: . Our goal is to update parameters to decrease a loss, which represents how the wrong model was so it would be next time more accurate. If we calculate derivative we can get something like in the graph above (probably with more dimensions). Let&#39;s take that red dot as one of the weights (parameters) and calculate the derivative of loss with respect to the parameters and we get something that can be represented like this: . . Important: It is not obvious, but by decreasing the parameters, we cause some parameters to be less important than the others, and with different inputs, each parameter will have different impact on the result. . Now we know the slope, it just needs some push to the right, and for that, we use learning rate and optimizer later. . We are interested only in the three types of derivatives function. Those are Linear, ReLU, and LogSoftmax. Also, we will need for LogSoftmax a derivative one-hot encoded label (target), so how do we even compute derivatives? . Remember that derivatives are just slopes. If we take, for example, a linear function like this one: . def linear_function(x, a=-2, b=-1): return a * x + b . To get slope we take some two coordinations on the line like (x=-2, y=2.5) and (x=-4, y=5) and calculate it: . slope = (y2 - y1) / (x2 - x1) slope = (-5 - 2.5) / (-4 -2) slope = -1.25 . Result is -1.25; that means if we take some x coordination like, for example, x2 and multiply it with the slope, we get y2 coordination. We then use that slope to move around its gradient descent (the first graph in this section) to find a global minimum, but we will have far more complex functions to derivate, and to be accurate, we pick as second coordinate something that is closest to first picked coordinations. . This isn&#39;t much elegant solution, and because of that, neural network libraries use Leibniz Notations because they calculate slope instantly (without second coordinates), which is far accurate and faster. To demonstrate it: . y = 2x y&#39; = d/dx * 2x y&#39; = d/d * 2 y&#39; = 2 . You can read more about it here here or I recommend Khan Academy. . Now is time to implement it for our functions. You can checkout all activation functions and their derivatives here (checkout there sigmoid function and its derivative function to spot their differencies and remember that derivative function is a slope of original funciton). . Some derivative need to remember input from the forward pass, and because we start with a loss (from loss function), which is &quot;at the end of the forward pass,&quot; we need to compute backward, which we will compute it as a chain that put the result of the previous derivative to another (those are called gradients). . If you want to know in detail the derivative of softmax. For linear (w1 and w2), it is just the input from the original function multiplied by the previous gradiant output (remember that, we use two linear layers). . For ReLU it is: . f(x) = 0 if x&#39; &lt; 0 x if x&#39; &gt;= 0 . where x is the gradient (output from the previous derivation) and x&#39; is input from w1 | . Luckily implementation in the code is fairly easy: . X = X_train[1337].reshape(1, 784) Y = np.array(Y_train[1337]).reshape(-1) # &lt;- target ## backward pass # target -&gt; one-hot encoded out = np.zeros((len(Y), 10), np.float32) out[range(out.shape[0]), Y] = 1 # logsoftmax loss function x_lsm = logsoftmax(x_w2) # Chain of derivates # derivative of target d_out = -out / len(Y) # derivative of Softmax with respect to target dx_lsm = d_out - np.exp(x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 d_w2 = np.dot(x_relu.T, dx_lsm) # derivative of ReLU d_relu = np.dot(dx_lsm, w2.T) d_relu[x_w1 &lt; 0] = 0 # derivative of l1 d_w1 = np.dot(X.T, d_relu) . There is only one last missing piece. We need to update parameters and for that, we use optimizer. . . Note: If you are still struggling with this part, then I recommend this video series from 3blue1brown, where gradient descent and backpropagation is visualized. You can compare it side by side with the code written so far. . Optimizer . We will use a simple one called SGD (Stochastic gradient descent). We will need a learning rate to say derivatives of parameters how big step we want to do in the gradient descent and update parameters. . lr = 0.001 # SGD w1 += -lr * d_w1 w2 += -lr * d_w2 . That&#39;s all! We just need to construct a training loop and also batches of data. . Prepare data in a batches . So far we have been reshaping data with the shape (1, 784), but we will be computing multiple images at once for better results to achieve better generalizing. We initiate batch randomly. . batch_size = 128 samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] . Commonly is used batch sizes of 64, 128, 256. We have to be careful with batch sizes and learning rate; else we could overfit the model. That means our model would &quot;memorize&quot; and not generalize, which would result in worse performance on new unseen data (you can try it by initiating batches orderly). . Constructing training loop . When we get output from our model we treat it as one-hot encoded array and we transform it to a single number than we compare it with labels (targets) to keep track of accuracy. Also, we will track loss (which we need to sum together). After the training loop, we will plot losses and accuracies. . output = x_w2 cat = np.argmax(output, axis=1) # from (10,) to (1) acc = (cat == Y).mean() x_loss = (-out * x_lsm).mean(axis=1) loss = x_loss.mean() loss, acc, cat . (0.12833504, 0.09375, array([7], dtype=int64)) . Let&#39;s construct everything together and run it: . w1 = _uniform(784, 128) w2 = _uniform(128, 10) lr = 0.001 batch_size = 128 losses, accuracies = [], [] ## Train for i in (t := trange(10000)): # Batch of training data &amp; target data samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] ## Forward pass x_w1 = linear(X, w1) x_relu = relu(x_w1) x_w2 = linear(x_relu, w2) output = x_w2 ## backward pass # target -&gt; one-hot encoded out = np.zeros((len(Y),10), np.float32) out[range(out.shape[0]), Y] = 1 # logsoftmax loss function x_lsm = logsoftmax(x_w2) # loss x_loss = (-out * x_lsm).mean(axis=1) loss = x_loss.mean() # derivative of target d_out = -out / len(Y) # derivative of Softmax with respect to target dx_lsm = d_out - np.exp(x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 d_w2 = np.dot(x_relu.T, dx_lsm) # derivative of ReLU d_relu = np.dot(dx_lsm, w2.T) d_relu[x_w1 &lt; 0] = 0 # derivative of l1 d_w1 = np.dot(X.T, d_relu) ## Update # SGD w1 += -lr * d_w1 w2 += -lr * d_w2 # Save for statistic cat = np.argmax(output, axis=1) acc = (cat == Y).mean() accuracies.append(acc) losses.append(loss) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) plt.ylim(-0.1, 1.1) plt.plot(losses) plt.plot(accuracies) plt.legend([&quot;losses&quot;, &quot;accuracies&quot;]) plt.show() . Loss: 0.00076; Acc: 1.00000: 100%|██████████| 10000/10000 [00:37&lt;00:00, 266.93it/s] . It looks good, but we need to evaluate it. We do it by inputting the whole testing set. We don&#39;t need the backward pass anymore if we don&#39;t plan to update it. . # Forward pass x_w1 = linear(X_test.reshape((-1, 28*28)), w1) x_relu = relu(x_w1) output = linear(x_relu, w2) # Measure accuracy Y_test_preds = np.argmax(output, axis=1) true_acc = (Y_test == Y_test_preds).mean() print(f&quot;Accuracy on testing set: {true_acc}&quot;) . Accuracy on testing set: 0.9793 . We have successfully build a neural network from scratch that evaluates around the human level. Here is the whole code: (I have edited little the code to resemble PyTorch example): . #hide-output import numpy as np import matplotlib.pyplot as plt from tqdm import trange def fetch(url): import requests, gzip, os, hashlib # source: http://yann.lecun.com/exdb/mnist/ # modified from: https://github.com/geohot/ai-notebooks/blob/master/mnist_from_scratch.ipynb filename = url.split(&quot;/&quot;)[-1] full_filename = &quot;dataset/mnist/&quot; + f&quot;{filename}&quot; if os.path.isfile(full_filename): with open(full_filename, &quot;rb&quot;) as f: dat = f.read() else: if not os.path.isdir(&quot;dataset&quot;): os.mkdir(&quot;dataset&quot;) if not os.path.isdir(&quot;dataset/mnist&quot;): os.mkdir(&quot;dataset/mnist&quot;) with open(full_filename, &quot;wb&quot;) as f: dat = requests.get(url).content f.write(dat) return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy() def mnist_dataset(): X_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;)[8:] X_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;)[8:] return (X_train, Y_train, X_test, Y_test) X_train, Y_train, X_test, Y_test = mnist_dataset() def _uniform(a, b, dtype=np.float32): return np.random.uniform(-1., 1., size=(a, b)).astype(dtype) / np.sqrt(a*b) def linear(a, b): return np.dot(a, b) def relu(x): return np.maximum(x, 0) def logsoftmax(x): return x - np.log(np.exp(x).sum(axis=1)).reshape((-1, 1)) class Net: def __init__(self, lr=0.001): # parameters self.w1 = _uniform(784, 128) self.w2 = _uniform(128, 10) self.lr = lr def calc_loss(self): # logsoftmax loss function self.x_lsm = logsoftmax(self.x_w2) self.loss = (-self.out * self.x_lsm).mean(axis=1).mean() def calc_optim(self): # Update (SGD) self.w1 += -self.lr * self.d_w1 self.w2 += -self.lr * self.d_w2 def forward(self, X): self.X = X self.x_w1 = linear(X, self.w1) self.x_relu = relu(self.x_w1) self.x_w2 = linear(self.x_relu, self.w2) return self.x_w2 def backward(self, Y): # target -&gt; one-hot encoded self.out = np.zeros((len(Y),10), np.float32) self.out[range(self.out.shape[0]), Y] = 1 # loss function self.calc_loss() # derivative of target d_out = -self.out / len(Y) # derivative of Softmax with respect to target dx_lsm = d_out - np.exp(self.x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 self.d_w2 = np.dot(self.x_relu.T, dx_lsm) # derivative of ReLU d_relu = np.dot(dx_lsm, self.w2.T) d_relu[self.x_w1 &lt; 0] = 0 # derivative of l1 self.d_w1 = np.dot(self.X.T, d_relu) net = Net() batch_size = 128 losses, accuracies = [], [] ## Train for i in (t := trange(10000)): # Batch of training data &amp; target data samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] output = net.forward(X) net.backward(Y) net.calc_optim() # Save for statistic cat = np.argmax(output, axis=1) # results from Net acc = (cat == Y).mean() accuracies.append(acc) losses.append(net.loss) t.set_description(f&quot;Loss: {net.loss:.5f}; Acc: {acc:.5f}&quot;) plt.ylim(-0.01, 1.1) plt.plot(losses) plt.plot(accuracies) plt.legend([&quot;losses&quot;, &quot;accuracies&quot;]) plt.show() ## Evaluation Y_test_preds = np.argmax(net.forward(X_test.reshape((-1, 28*28))), axis=1) true_acc = (Y_test == Y_test_preds).mean() print(f&quot;Accuracy on testing set: {true_acc}&quot;) . . You can compare it with the code from the PyTorch example and see differences. . Conclusion . I hope it helped. We have made a neural network that classifies digits on a human level. When I was starting with neural networks, I was often confused with the concepts behind them and their implementations in the code, but the best way to learn is by doing and keep pushing. If you would like to continue, I recommend these resources (from which I have learned): . https://www.fast.ai | https://www.amazon.com/gp/product/B01EER4Z4G/ref=ppx_yo_dt_b_d_asin_title_o03?ie=UTF8&amp;psc=1 | https://www.amazon.com/gp/product/B085Z96M9P/ref=dbs_a_def_rwt_bibl_vppi_i1 | .",
            "url": "https://ludius0.github.io/my-blog/ai/deep%20learning%20(dl)/2020/12/14/Neural-network-from-scratch.html",
            "relUrl": "/ai/deep%20learning%20(dl)/2020/12/14/Neural-network-from-scratch.html",
            "date": " • Dec 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m ludius0. I have been programming almost year and I have been finishing my last high school year. I don’t have social media, but I have github account and youtube channel. . Also I generated this website with fastpages 1. . a blogging platform that natively supports Jupyter notebooks. It’s great check it out if you want also do blogging. &#8617; . |",
          "url": "https://ludius0.github.io/my-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ludius0.github.io/my-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}