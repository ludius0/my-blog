{
  
    
        "post0": {
            "title": "Learning rate finder",
            "content": "Learning rate (LR) is maybe the most important hyperparameter for our neural networks because it has the biggest impact on the result. Learning rate plays its role during backpropagation when we are moving parameters (like weights) to minimize a loss in gradient descent. If we choose a learning rate too high, it won&#39;t decrease to a global minimum and if our learning rate is too small, then it will get stuck in a local minimum. . Source: https://www.bdhammel.com/learning-rates/ . To find the right learning rate, we can use the learning rate finder. Idea is that we will start with a small learning rate, and with every training cycle we will increase it and see how it affected loss. . The type I will go with is exponential and I will try it on a small neural network designed for MNIST classification (wrote about it in the previous blog post). Code is written for Pytorch, but it can be easily applied in other deep learning libraries. . So first, we have to import some things. . Preparing Imports, Data and Model . Import modules: . import torch # DL library import torch.nn as nn import torchvision.datasets as datasets # get dataset from tqdm.notebook import trange, tqdm # for fancy loading bar from pathlib import Path . Get dataset . mnist_trainset = datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=None) mnist_testset = datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=None) X_train, Y_train = mnist_trainset.data.float(), mnist_trainset.targets X_test, Y_test = mnist_testset.data.float(), mnist_testset.targets . Define model (and create training loop) . torch.manual_seed(1337) # set randomness class TorchNet(nn.Module): def __init__(self): super(TorchNet, self).__init__() self.l1 = nn.Linear(784, 256, bias=False) self.l2 = nn.Linear(256, 10) self.sm = nn.LogSoftmax(dim=1) self.act = nn.ReLU() def forward(self, x): x = self.act(self.l1(x)) x = self.sm(self.l2(x)) return x model = TorchNet() loss_f = nn.NLLLoss(reduction=&#39;none&#39;) optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0) BS=128 . Save untrained parameters to load them later and compare a results. . PATH = Path(&quot;model_data&quot;) torch.save({&#39;model_state_dict&#39;: model.state_dict()}, PATH) . losses, accuracies = [], [] for i in (t := trange(1000)): # Create Batch (randomly initiated) samp = torch.randint(len(mnist_trainset), (BS,)) X, Y = X_train[samp].reshape(BS, 28*28), Y_train[samp] # Pass through Net (+ calculate loss) output = model(X) loss = loss_f(output, Y).mean() # Update parameters optim.zero_grad() loss.backward() optim.step() # Save for statistic cat = torch.argmax(output, dim=1) acc = (cat == Y).float().mean() accuracies.append(acc.item()) losses.append(loss.item()) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) ## Evaluation Y_test_preds = torch.argmax(model.forward(X_test.reshape((-1, 28*28))), dim=1) true_acc = (Y_test == Y_test_preds).float().mean() print(f&quot;Accuracy on testing set: {100 * true_acc:.5f}%&quot;) . . Accuracy on testing set: 94.00000% . this neural network during training (in 1000 range loop) is able to train to 94% accuracy, but we can find a better learning rate for this model. . Creating learning rate finder (exponential) . First, we need to define a range, where we will be looking for optimal learning rate: . start_lr = 1e-6; end_lr = 1 . we can define it in find_lr function, we will also need to input to the function: model, loss function, optimizer, and batch (BS) because we will be updating the model during loop and track loss for selected learning rate. . def find_lr(model, loss_f, optim, start_lr=10e-7, end_lr=1, BS=128): if start_lr &gt; end_lr: return loss_list, lr_list = [], [] . We need to define a number of iteration, it will set how many learning rates we will try. This number can be chosen manually for own preference, but we can define it dependently on the length of training size divided by batches. . num_iter = len(X_train) // BS num_iter . 468 . We then create lr_factor, which by multiplying with starting learning rate will be increasing and we can create a list of all learning rates from starting learning rate to ending learning rate in the range of the number of iterations. For that, we need to use exponential and logarithm functions (this concept is similar to the Softmax function). . from math import exp, log . lr_factor = exp(log(end_lr / start_lr) / num_iter) lrs = [start_lr] for _ in range(num_iter): lrs.append(lrs[-1] * lr_factor) . If we check it, . len(lrs), (lrs[0], lrs[-1]), lrs[0:4], lrs[0]*lr_factor . (469, (1e-06, 1.0000000000000153), [1e-06, 1.02996036580999e-06, 1.0608183551394486e-06, 1.0926008611173784e-06], 1.02996036580999e-06) . we can see that length is the same as num_iter, it start and end with our defined range and by multiplying with lr_factor, we are increasing it. . We just need to recreate the training loop, update the learning rate every iteration, and track loss with the learning rate. . Also, if we want to analyze effectively, we need to load untrained parameters and compare the results. . PATH = Path(&quot;model_data&quot;) parameters = torch.load(PATH) model.load_state_dict(parameters[&#39;model_state_dict&#39;]) . for curr_lr in lrs: optim.param_groups[0][&#39;lr&#39;] = curr_lr # update learning rate in optimizer # Create Batch (randomly initiated) samp = torch.randint(len(mnist_trainset), (BS,)) X, Y = X_train[samp].reshape(BS, 28*28), Y_train[samp] # Pass to NN output = model(X) loss = loss_f(output, Y).mean() # Update parameters optim.zero_grad() loss.backward() optim.step() # Save data loss_list.append(log(loss.item())) . Not much has changed, we also use log operation for loss to get smaller values to have a smaller graph later. Because we have already a list of all learning rates (lrs), we don&#39;t need to keep track of it. . Let&#39;s plot it: . import matplotlib.pyplot as plt . def plot_lr_find(loss_list, lr_list): fig, ax = plt.subplots(1,1) ax.plot(lr_list, loss_list) ax.set_ylabel(&quot;Loss&quot;) ax.set_xlabel(&quot;Learning Rate (log scale)&quot;) ax.set_xscale(&#39;log&#39;) plt.show() . plot_lr_find(loss_list, lrs) . We are looking for an area, where is the biggest decrease in loss (steepest point). It means it &quot;jump&quot; most optimally in gradient descent. It doesn&#39;t seem obvious, but the area between 10e-4 and 10e-3 should be best (the area between 10e-2 to 10e-0 is worst because it&#39;s too high to fall into the global minimum or decent local minimum). A good rule of thumb is to choose the lowest point and divide it by ten or we can find the steepest point by computing slope, which takes two points (coordination). . $$slope = frac{y2-y1}{x2-x1} $$ . We can do both effectively in NumPy. . import numpy as np . def get_steepest_point(losses, lrs): losses, lrs = np.array(loss_list), np.array(lrs) grads = (losses[1:]-losses[:-1]) / (np.log(lrs[1:])-np.log(lrs[:-1])) return (lrs[grads.argmin()].item(), lrs[losses.argmin()].item()/10.) . . Note: We already have saved losses after logarithm operations, so we need to use np.log on learning rates. . steepest_p, lowest_p = get_steepest_point(losses, lrs) steepest_p, lowest_p . (0.08376776400683023, 0.0008376776400683004) . You can now load untrained parameters and compare the results from training. . After some experimenting I have found that even the steepest point should be divided by ten and this method can be quite inaccurate for small neural networks (like with only one hidden layer), so if we would scale up our model (with hidden layers), . class TorchNet(nn.Module): def __init__(self): super(TorchNet, self).__init__() self.l1 = nn.Linear(784, 1024, bias=False) self.l2 = nn.Linear(1024, 512, bias=False) self.l3 = nn.Linear(512, 256, bias=False) self.l4 = nn.Linear(256, 128, bias=False) self.l5 = nn.Linear(128, 64, bias=False) self.l6 = nn.Linear(64, 10) self.sm = nn.LogSoftmax(dim=1) self.act = nn.ReLU() def forward(self, x): x = self.act(self.l1(x)) x = self.act(self.l2(x)) x = self.act(self.l3(x)) x = self.act(self.l4(x)) x = self.act(self.l5(x)) x = self.sm(self.l6(x)) return x . . we get more accurate results . Steepest: 0.0146780; Lowest: 0.0151178 . After I plugged the steepest point I got a better accuracy of around 2%. It seems a small change, but keep in mind that I have already guessed a strong learning rate before and this method yields me an even better learning rate. . Alternative (One-cycle learning rate finder) . The idea of this LR finder is to generate learning rates in a cycle (in epochs), so the learning rate would get bigger and then back smaller and code for it is something like this. . class OneCycleScheduler(): def __init__(self, iterations, max_rate, start_rate=None, last_iterations=None, last_rate=None): self.iterations = iterations self.max_rate = max_rate self.start_rate = start_rate or max_rate / 10 self.last_iterations = last_iterations or iterations // 10 + 1 self.half_iteration = (iterations - self.last_iterations) // 2 self.last_rate = last_rate or self.start_rate / 1000 self.iteration = 0 def _interpolate(self, iter1, iter2, rate1, rate2): return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1) def on_batch_begin(self): if self.iteration &lt; self.half_iteration: rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate) elif self.iteration &lt; 2 * self.half_iteration: rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate) else: rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate) rate = max(rate, self.last_rate) self.iteration += 1 return rate . So if we would like to check the generations of learning rates, . n_epochs = 25 iter_ = len(X_train) // BS * n_epochs onecycle = OneCycleScheduler(iter_, max_rate=10e-1, start_rate=10e-7) check = [] for idx in range(iter_): curr_lr = onecycle.on_batch_begin() check.append(curr_lr) print(check[::200]) . [1e-06, 0.0379948829787234, 0.0759887659574468, 0.11398264893617022, 0.1519765319148936, 0.18997041489361702, 0.22796429787234043, 0.26595818085106376, 0.3039520638297872, 0.3419459468085106, 0.379939829787234, 0.4179337127659574, 0.45592759574468084, 0.4939214787234042, 0.5319153617021276, 0.569909244680851, 0.6079031276595744, 0.6458970106382979, 0.6838908936170213, 0.7218847765957447, 0.7598786595744681, 0.7978725425531915, 0.8358664255319149, 0.8738603085106383, 0.9118541914893618, 0.9498480744680852, 0.9878419574468085, 0.9741641595744681, 0.9361702765957447, 0.8981763936170213, 0.8601825106382979, 0.8221886276595745, 0.784194744680851, 0.7462008617021276, 0.7082069787234042, 0.6702130957446808, 0.6322192127659575, 0.5942253297872341, 0.5562314468085107, 0.5182375638297873, 0.48024368085106384, 0.4422497978723404, 0.4042559148936171, 0.3662620319148937, 0.3282681489361703, 0.2902742659574469, 0.25228038297872346, 0.21428650000000005, 0.17629261702127663, 0.13829873404255322, 0.1003048510638298, 0.06231096808510639, 0.024317085106382974, 9.386279863481228e-07, 7.681501706484642e-07, 5.976723549488055e-07, 4.271945392491468e-07, 2.5671672354948807e-07, 8.623890784982944e-08] . when implemented, it gives us similar results (also here I don&#39;t divide steepest point by ten). . def get_steepest_point(losses, lrs): grads = (losses[1:]-losses[:-1]) / (np.log(lrs[1:])-np.log(lrs[:-1])) return (lrs[grads.argmin()].item(), lrs[losses.argmin()].item()/10.) def find_lr(model, loss_f, optim, start_lr=10e-7, end_lr=10e-3, BS=256, n_epochs=25): if start_lr &gt; end_lr: return loss_list, lr_list = [], [] iter_ = len(X_train) // BS * n_epochs onecycle = OneCycleScheduler(iter_, max_rate=end_lr, start_rate=start_lr) for _ in (t := trange(iter_)): curr_lr = onecycle.on_batch_begin() # rewrite lr optim.param_groups[0][&#39;lr&#39;] = curr_lr lr_list.append(curr_lr) # Create Batch (randomly initiated) samp = torch.randint(len(mnist_trainset), (BS,)) X, Y = X_train[samp].reshape(BS, 28*28), Y_train[samp] # Pass to NN output = model(X) loss = loss_f(output, Y).mean() # Update parameters optim.zero_grad() loss.backward() optim.step() # Save data loss_list.append(log(loss.item())) losses = np.array(loss_list[:-5]) lrs = np.array(lr_list[:-5]) steepest, lowest = get_steepest_point(losses, lrs) print(f&quot;Steepest: {steepest:.7f}; Lowest: {lowest:.7f}&quot;) plot_lr_find(losses, lrs) find_lr(model, loss_function, optim) . . Steepest: 0.0097227; Lowest: 0.0000000 . . Note: It seems that the lowest point cannot be applied for this method . Conclusion . I hope it helped as an introduction for the learning rate finder. DL libraries like Pytorch and others have already implemented LR finders or you can implement them from scratch (I have used code from here) and you can check original paper. The first DL framework implementing LR finder was fast.ai and it is most easy to use. . I have tested it on MNIST classification, but generally, it should be applicable for other types of neural networks. One-cycle seems to be more accurate (for bigger models), but also they are more computation consuming. For exponential LR finder, I have found that the steepest point (divided by ten) is more accurate than the smallest point divided by ten, but they blend together with bigger models. . Overall it is far better than guessing. .",
            "url": "https://ludius0.github.io/my-blog/deep%20learning%20(dl)/2021/01/24/Learnig-rate-finder.html",
            "relUrl": "/deep%20learning%20(dl)/2021/01/24/Learnig-rate-finder.html",
            "date": " • Jan 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Neural network from scratch",
            "content": "To have a genuine understanding of neural networks is valuable because they feel like &quot;black boxes.&quot; To be sure we understand it, we should build one from scratch, so I decided to create one and write about it. In this post, I will build one from scratch for classifying MNIST dataset and touch on underlying concepts behind it. I hope it helps. . Prerequisities . Code is written in python 3.8+ and I use the library called NumPy for matrixes multiplication. The math behind neural networks can look daunting, but it is basically high school math. . This is everything I am going to import for building NN: . import numpy as np import matplotlib.pyplot as plt # for plotting arrays from tqdm import trange # for fancy loading console output . NumPy . Neural networks use tensor for passing multiple data through a network. In Python, we don&#39;t have tensors, but those can be represented with arrays from NumPy library. It provides n-dimensional arrays, faster computing (broadcasting), cleaner code, a few functions we&#39;re gonna use and it is commonly used for data science. If you&#39;re not familiar with NumPy, you can check this video to learn about it (or read about it). Numpy is there explained in details and to plot arrays, I will use its parent library Matplotlib. . MNIST dataset . Before we start to build one, we have to import the dataset to work with. For that we need this code: . import requests, gzip from pathlib import Path # source: http://yann.lecun.com/exdb/mnist/ # modified from: https://github.com/geohot/ai-notebooks/blob/master/mnist_from_scratch.ipynb def fetch(url): name = url.split(&quot;/&quot;)[-1] dirs = Path(&quot;dataset/mnist&quot;) path = (dirs / name) if path.exists(): with path.open(&quot;rb&quot;) as f: dat = f.read() else: if not dirs.is_dir(): dirs.mkdir(parents=True, exist_ok=True) with path.open(&quot;wb&quot;) as f: dat = requests.get(url).content f.write(dat) return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy() def mnist_dataset(): X_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;)[8:] X_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;)[8:] return (X_train, Y_train, X_test, Y_test) . MNIST dataset is made from 70000 images. It is split in two types: training (60k) &amp; testing (10k) data. Each image has 28 to 28 pixels and its own label, which describes it. . X_train, Y_train, X_test, Y_test = mnist_dataset() select = 1337 plt.imshow(X_train[select]) print(f&quot;Number: {Y_train[select]} | Shape: {X_train[select].shape}&quot;) . Number: 6 | Shape: (28, 28) . The reason we have a split dataset is for validating how (if even) our network functions properly. We will be passing images as flatten images through the neural network to &quot;show it&quot; the whole image. Right now we are going to pass only one image, but later we will be passing more at once as batch. So shape is: . select = 1337 plt.imshow(X_train[select].reshape(1, -1)) print(f&quot;Number {Y_train[select]} | Shape: {X_train[select].reshape(1, -1).shape}&quot;) . Number 6 | Shape: (1, 784) . . Note: We have that shape because it is only one image and 28 (pixels) multiply by 28 equal 784. . How it looks like? . To have some visual image of what we&#39;re going to build: . Source: https://github.com/fastai/fastbook/blob/master/01_intro.ipynb . . Important: Through out this blog post I am using termininology in this image. . This is how a Neural Network (NN) looks like! Our inputs are MNIST images in the form of a NumPy 2D array (matrix), which contains numbers between 0 to 1. Usually, we represent data between 0 to 1 or -1 to 1 (It is an unwritten golden rule). Architecture (model) is basically our neural network. There we pass inputs through nodes (neurons), which compute some operations with parameters and we get predictions. We then take labels (targets) and compare them with our prediction. We measure how wrong our model was with the loss function and use it to update parameters so it could &quot;learn&quot;. . Maybe now is a good question what is even neural network?. It is inspired by neurons in our brain. Neuron &quot;fire&quot; signal if it gets a certain amount of input and millions of them can create complex structures. For our case, we won&#39;t need millions of them, but only a few. Right now there are three types of structure in the field: linear, convolutional, and recurrent. We are going to use the simplest and oldest one the linear layer: . outputs = weights * inputs + biases . As we can see it is a linear function. In our model, we won&#39;t use biases, so weights are the only parameters we will be updating in the training loop. . . Note: With this, our model will be learning on its own and when our programming function is creating its own function, we call it machine learning. . So far, so good, but to clarify how our architecture will look like: every node (neuron) will be a linear function, and we stack them as a full layer and pass every input to every node, where each contains a parameter. We then stack multiple layers. . This image (below) roughly show how a neural network looks like. Here it is defined with: input layer (16 nodes), hidden layer 1 (12 nodes), hidden layer 2 (12 nodes), output layer (1 node). . Created with: https://alexlenail.me/NN-SVG/index.html . Each of those &quot;strings&quot; are weights (parameter) and we can see that every node has its own weight for every node from the previous layer. We will create only one hidden layer with 128 nodes. Our input layer is flattened, where each pixel is represented as one node in the input layer. The last layer is the output layer, and because we classify 0-9 images it will have 10 nodes. . . Note: When a neural network has more hidden layers we call it deep learning (which is a subset field of machine learning). . PyTorch example . To show you everything we will be implementing from scratch I have prepared an example made with a popular deep learning library called PyTorch. Just hit the &quot;show code&quot; button to have a picture of what in the code we are going to implement (from scratch). . #hide-output import torch import torch.nn as nn torch.manual_seed(7) class TorchNet(nn.Module): def __init__(self): super(TorchNet, self).__init__() self.l1 = nn.Linear(784, 128, bias=False) self.l2 = nn.Linear(128, 10) self.sm = nn.LogSoftmax(dim=1) self.act = nn.ReLU() def forward(self, x): x = self.act(self.l1(x)) x = self.l2(x) x = self.sm(x) return x model = TorchNet() loss_function = nn.NLLLoss(reduction=&#39;none&#39;) optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0) BS = 128 losses, accuracies = [], [] for i in (t := trange(10000)): # Create Batch (randomly initiated) samp = np.random.randint(0, X_train.shape[0], size=(BS)) X = torch.tensor(X_train[samp].reshape((-1, 28*28))).float() Y = torch.tensor(Y_train[samp]).long() # Pass through Net (+ calculate loss) output = model.forward(X) loss = loss_function(output, Y).mean() # Update parameters optim.zero_grad() loss.backward() optim.step() # Save for statistic cat = torch.argmax(output, dim=1) acc = (cat == Y).float().mean() accuracies.append(acc.item()) losses.append(loss.item()) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) . . It evaluated about 98% on the testing set. We will be recreating this whole code from scratch. I recommend to use this section as reference. . Neural network structure . As the first image show in the How it looks like?, we will be implementing everything that is there. The whole code for the neural network should be under 100 lines of code in Python. . Parameters (Weights) . We&#39;re going to have three layers: input layer (784 nodes), hidden layer (128 nodes), and output layer (10 nodes). We pass input to the first hidden layer and output will be pass forward to another layer to the next layer until it reaches the final layer (output layer). Each node will contain its own parameter, which will perform some operation with the input. We are going to use a simple linear layer (only weights), so our parameters will be only one array for each layer. . Every neural network library uses its own initiate functions that create usually numbers between -1 to 1 or 0 to 1. I chose uniform function because it performed best from what I have observed. . def init_uniform(a, b, dtype=np.float32): return np.random.uniform(-1., 1., size=(a, b)).astype(dtype) / np.sqrt(a*b) . . Note: We&#8217;re going to use np.float32 for accuracy. All popular neural network libraries use them by default. . w1 = init_uniform(784, 128) # layer one (weights) | represent input layer &amp; hidden layer 1 w2 = init_uniform(128, 10) # layer two (weights) | represent hidden layer 1 &amp; output layer w1.shape . (784, 128) . In the code, weights (w1 and w2) are representing whole layer. We can see in init_uniform(784, 128) and init_uniform(128, 10) we input the whole all three layers of our model. . Architecture (Linear Layer &amp; Activation function) . We are representing linear layers as inputs multiply by weights. To perform our linear pass we take one image from the training set and use @ Python operator that multiply rows of first array with columns of second array. Alternatively in Numpy you can use np.dot(a, b). . linear = lambda a, b: a @ b . img = X_train[0].reshape(-1) x_l1 = linear(img, w1) x_l1.shape . (128,) . Just to clarify how linear function looks like: . With linear layers, we use an activation functions. They are non-linear function. We use them to have non-linear outputs. Imagine your input data are, for example, sinus. If our architecture would be only made by a linear function (linear layers), it couldn&#39;t learn how to &quot;bend&quot; output to something similar to the sinus. . . Note: Another reason we use them is the universal approximation theorem. Basically, with only two linear layers, we could rewrite it as one linear layer, but with a non-linearity (in our case between them) and sufficiently big matrices, it can approximate any arbitrary function. . We use it after the hidden layer. Because we only classify It isn&#39;t a necessity, but we still get better performance. We implement ReLU (Rectified linear unit), which is most used: . $$f(x)=max(0,x)$$ . relu = lambda x: np.maximum(x, 0) . It looks like this: . Implementation with our code: . x_w1 = linear(X_train[0].reshape(1, -1), w1) x_relu = relu(x_w1) x_w2 = linear(x_relu, w2) x_w2 . array([[-0.6520123 , -0.24983323, -0.6080282 , -0.24861592, -0.35031724, 0.60303384, -0.5354343 , -0.23014745, -0.36763242, 0.1088623 ]], dtype=float32) . That&#39;s it. We have a forward pass for our neural network. After we pass the image array, we get an output of size 10. But the real question is How it will learn?. . Loss function . To teach neural network we have to figure out how to represented how big is an error (difference) between prediction and label (target). . First, we need to get prediction (output) from the forward pass. We pass one of the images through our layers, that will give us an array of shape (10,). With the Numpy function np.argmax() we can extract that as a single number by the position of the biggest number in the array. . select = 1337 Y = Y_train[select] # target / label X = X_train[select].reshape(1, -1) # input # Forward pass x_w1 = linear(X, w1) x_relu = relu(x_w1) x_w2 = linear(x_relu, w2) # output output = np.argmax(x_w2) # Take biggest number in array (by position) # Differences print(f&quot;Correct number: {Y}&quot;) print(f&quot;Prediction: {output}&quot;) . Correct number: 6 Prediction: 5 . We can see its prediction is wrong. We need to calculate error between output and label. Our output x_w2 is already array, so we need to transform label to array of same shape like x_w2, which is (10,). That can be done by using one-hot encoded array. . label_arr = np.zeros((10, ), np.float32) label_arr[Y] = 1 # Differences print(f&quot;Correct number: {label_arr} -&gt; {Y}&quot;) print(f&quot;Prediction: {x_w2} -&gt; {output}&quot;) . Correct number: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] -&gt; 6 Prediction: [[-1.1656513 -0.39488536 -0.83915067 -0.42164934 0.39440176 1.1336713 0.01601482 -1.4128247 0.02184126 0.12216576]] -&gt; 5 . We will calculate the error during backpropagation, now we need to transform the output to loss, which &quot;describe better&quot; the difference between label and prediction. Because we are classifying single digits, we have 10 categories. For that, we will use Cross-Entropy Loss. We want NN output to be a distribution of how much sure it is, which can be represented as a percentage between 0% to 100%. For that, we need transform (squish) data of prediction between 0 to 1. Activation function which handles that is called Softmax. . $$ sigma(z)_i = frac{e^{z_i}}{ sum_{j=1}^Ke^{z_j}} $$ . softmax = lambda x: np.exp(x) / np.exp(x).sum(axis=1) . . Note: Exponencial operation -&gt; np.exp(x) is defined as e ** x, where e is special (magical) number like pi. Its value is approximately equal to 2.718. . softmax(x_w2) . array([[0.03093783, 0.06686967, 0.0428832 , 0.06510371, 0.1472354 , 0.30837056, 0.10085116, 0.0241626 , 0.10144047, 0.11214544]], dtype=float32) . If we sum Softmax together, we get single digit. . softmax(x_w2).sum() . 1.0 . The function will calculate the probabilities of each label class over all possible label classes (useful for classifying -&gt; multi-category array). With this, we can get the confidence of our neural network. . s_out = softmax(x_w2) print(f&quot;{round(100 * s_out[0][output], 4)}%&quot;) . 30.8371% . Right now it is not confident about its own result. We will later in Backpropagation compare it to our one-hot encoded label (target) to say neural network how wrong it was. . But the problem with Softmax is that during Backpropagation, neural network wouldn&#39;t &quot;see&quot; the difference between 0.99 and 0.999 despite that it is 10 times bigger. To magnify this difference, we will use Negative Log-Likelihood (NLL), which uses a logarithm. . nll_loss = lambda x: np.log(x) . . Important: I don&#8217;t use a negative logarithm for computational reason later in the backward pass. . Logarithm is inverted exponencial: . $$z = x^y$$ $$y = log{_x}{z}$$ . . Note: Before computers, we used it to calculate the largest or smallest numbers. . Our biggest number will become our smallest number in the array, and the difference between other numbers will be more significant. . loss = nll_loss(s_out) print(f&quot;Softmax: {s_out} nSum: {s_out.sum()} n&quot;) print(f&quot;NLL: {loss} nSum:: {loss.sum()}&quot;) . Softmax: [[0.03093783 0.06686967 0.0428832 0.06510371 0.1472354 0.30837056 0.10085116 0.0241626 0.10144047 0.11214544]] Sum: 1.0 NLL: [[-3.4757757 -2.7050097 -3.149275 -2.7317739 -1.9157226 -1.1764531 -2.2941096 -3.7229493 -2.288283 -2.1879587]] Sum:: -25.64731216430664 . We just need to bundle it together and we get Cross-Entropy Loss: . $$H_{ textrm{(p, q)}} = - sum_{x epsilon{X}}p(x) log{_q}(x)$$ . that can be written like this in a code: . crossentropyloss = lambda x: np.log(np.exp(x) / np.exp(x).sum(axis=1)) # NLL &amp; Softmax . We have our loss function, but we won&#39;t be implementing it in the forward pass. We use the loss function to update our parameters, which are w1 and w2, so it will be implemented in backward pass. . Stable code . As a side note, there is a computationally more stable way of the code of our loss function, which I will use. It&#39;s an explanation you can find here. . # and from: # http://gregorygundersen.com/blog/2020/02/09/log-sum-exp/ def logsumexp(x): c = x.max(axis=1) return x - (c + np.log(np.exp(x-c.reshape((-1, 1))).sum(axis=1))).reshape((-1, 1)) . It seems like it is different, but it results in the same output as our previous function: . loss1 = crossentropyloss(x_w2) loss2 = logsumexp(x_w2) loss1.sum(), loss2.sum() . (-25.647312, -25.647308) . Backpropagation . The fundamental question is How do you meaningfully update parameters?. The answer is we use derivatives of functions we implemented in architecture (model). Derivatives are representing the slope of the original function. All those results from each layer give us a multidimensional representation called gradient descent. We calculate them to find how much we need to increase or decrease parameters so we would decrease a loss and update parameters. This part is conceptually the hardest one to understand. . Let&#39;s say we have only one small linear hidden layer with a loss function and we get this: . Our goal is to update parameters to decrease a loss, which represents how the wrong model was so it would be next time more accurate. If we calculate derivative we can get something like in the graph above (probably with more dimensions). Let&#39;s take that red dot as one of the weights (parameters) and calculate the derivative of loss with respect to the parameters and we get something that can be represented like this: . . Important: It is not obvious, but by moving the parameters, we cause to some of them to be less important than the others, and with different inputs, each parameter will have different impact on the result. . Now we know the slope, it just needs some push to the right, and for that, we will use learning rate and optimizer later. . We are interested only in the three types of derivatives function. Those are Linear, ReLU, and LogSoftmax. Also, we will need for LogSoftmax a derivative one-hot encoded label (target), so how do we even compute derivatives? . Remember that derivatives are just slopes. If we take, for example, a linear function like this one: . def linear_function(x, a=-2, b=-1): return a * x + b . To get slope we take some two coordinations on the line like (x=-2, y=2.5) and (x=-4, y=5) and calculate it: . $$slope = frac{y2 - y1}{x2 - x1}$$ $$slope = frac{-5 - 2.5}{-4 -2} $$ $$slope = -1.25$$ . Result is -1.25; that means if we take some x coordination like, for example, x2 and multiply it with the slope, we get y2 coordination. We then use that slope to move around its gradient descent (the first graph in this section) to find a global minimum (lowest point), but we will have far more complex functions to derivate, and this method would be for them inaccurate like here: . This isn&#39;t much elegant solution, and because of that, neural network libraries use Leibniz Notations because they calculate slope instantly (without second coordinates). To demonstrate it: . $${y = 2x} $$ $${y&#39; = frac{d}{dx} * 2x} $$ $${y&#39; = frac{d}{d} * 2} $$ $${y&#39; = 2}$$ . You can read more about it here here or I recommend Khan Academy. . Now is time to implement it for our functions. You can checkout all activation functions and their derivatives here. . Some derivative need to remember input from the forward pass, and because we start with a loss (from loss function), which is &quot;at the end of the forward pass,&quot; we need to compute backward, which we will compute it as a chain that put the result of the previous derivative to another (those are called gradients). . If you want to know in detail the derivative of Softmax &amp; NLL (keep in mind I&#39;m using logsumexp). For linear (w1 and w2), it is just the input from the original function multiplied by the previous gradiant output. . For ReLU it is: $$f&#39;(x) = 0 textrm{ (if } {x&#39; &lt; 0)} textrm{ else } textrm{x} textrm{ (if } {x&#39; &gt;= 0)} $$ . where x is the gradient (output from the previous derivation) and x&#39; is input from w1 | . We will also need to use .T Numpy function (transpose) that swap rows and columns, so we would have the correct shape for matrix multiplication. . Luckily implementation in the code of backpropagation is fairly easy: . X = X_train[1337].reshape(1, 784) Y = np.array(Y_train[1337]).reshape(-1) # &lt;- target ## backward pass # target -&gt; one-hot encoded out = np.zeros((len(Y), 10), np.float32) out[range(out.shape[0]), Y] = 1 # crossentropyloss x_lsm = logsumexp(x_w2) # Chain of derivates # derivative of target d_out = -out / len(Y) # derivative of loss with respect to target dx_lsm = d_out - np.exp(x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 d_w2 = x_relu.T @ dx_lsm # derivative of ReLU d_relu = dx_lsm @ w2.T d_relu[x_w1 &lt; 0] = 0 # derivative of l1 d_w1 = X.T @ d_relu . There is only one last missing piece. We need to update parameters and for that, we use optimizer. . . Note: If you are still struggling with this part, then I recommend this video series from 3blue1brown, where gradient descent and backpropagation is visualized. You can compare it side by side with the code written so far. . Optimizer . We will use a simple one called SGD (Stochastic gradient descent). We will need a learning rate to say derivatives of parameters how big step we want to do in the gradient descent and update parameters. If our learning rate is too big, it will not reach global minimum (it will over shoot it). . If our learning rate is too small, it will get stuck at a local minimum (obviously we want global minimum). . Source: https://www.bdhammel.com/learning-rates/ . Forunet . # learning rate lr = 0.001 # SGD w1 += -lr * d_w1 w2 += -lr * d_w2 . . Note: The reasons for the learning rate to be that &quot;small&quot; is that our weights are initiated between -1 to 1. . That&#39;s all! We just need to construct a training loop and also batches of data. . Prepare data in a batches . So far we have been reshaping data with the shape (1, 784), but we will be computing multiple images at once for better results to achieve better generalizing. We initiate batch randomly. . batch_size = 128 samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] . Commonly is used batch sizes of 64, 128, 256. We have to be careful with batch sizes and learning rate; else we could overfit the model. That means our model would &quot;memorize&quot; and not generalize (optimal), which would result in worse performance on new unseen data (you can try it by initiating batches orderly). . Source: https://www.fastaireference.com/overfitting . Constructing training loop . When we get output from our model we treat it as one-hot encoded array and we transform it to a single number than we compare it with labels (targets) to keep track of accuracy. Also, we will track loss (which we need to sum together -&gt; to get single value). After the training loop, we will plot losses and accuracies. . output = x_w2 cat = np.argmax(output, axis=1) # from (10,) to (1) acc = (cat == Y).mean() x_loss = (-out * x_lsm).mean(axis=1) loss = x_loss.mean() loss, acc, cat . (0.22941096, 0.1171875, array([5], dtype=int64)) . Let&#39;s construct everything together and run it: . w1 = init_uniform(784, 128) w2 = init_uniform(128, 10) lr = 0.001 batch_size = 128 losses, accuracies = [], [] ## Train for i in (t := trange(10000)): # Batch of training data &amp; target data samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] ## Forward pass x_w1 = linear(X, w1) x_relu = relu(x_w1) x_w2 = linear(x_relu, w2) output = x_w2 ## backward pass # target -&gt; one-hot encoded out = np.zeros((len(Y),10), np.float32) out[range(out.shape[0]), Y] = 1 # crossentropyloss x_lsm = logsumexp(x_w2) # loss x_loss = (-out * x_lsm).mean(axis=1) loss = x_loss.mean() # derivative of target d_out = -out / len(Y) # derivative of loss with respect to target dx_lsm = d_out - np.exp(x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 d_w2 = x_relu.T @ dx_lsm # derivative of ReLU d_relu = dx_lsm @ w2.T d_relu[x_w1 &lt; 0] = 0 # derivative of l1 d_w1 = X.T @ d_relu ## Update # SGD w1 += -lr * d_w1 w2 += -lr * d_w2 # Save for statistic cat = np.argmax(output, axis=1) acc = (cat == Y).mean() accuracies.append(acc) losses.append(loss) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) plt.ylim(-0.1, 1.1) plt.plot(losses) plt.plot(accuracies) plt.legend([&quot;losses&quot;, &quot;accuracies&quot;]) plt.show() . Loss: 0.00142; Acc: 0.99219: 100%|██████████| 10000/10000 [00:41&lt;00:00, 240.13it/s] . It looks good, but we need to evaluate it. We do it by inputting the whole testing set. We don&#39;t need the backward pass anymore if we don&#39;t plan to update it. . # Forward pass x_w1 = linear(X_test.reshape((-1, 28*28)), w1) x_relu = relu(x_w1) output = linear(x_relu, w2) # Measure accuracy Y_test_preds = np.argmax(output, axis=1) true_acc = (Y_test == Y_test_preds).mean() print(f&quot;Accuracy on testing set: {true_acc}&quot;) . Accuracy on testing set: 0.9798 . We have successfully build a neural network from scratch that evaluates around the human level. Here is the whole code: (I have edited little the code to resemble more PyTorch example): . #hide-output import numpy as np import matplotlib.pyplot as plt from tqdm import trange import requests, gzip from pathlib import Path def fetch(url): name = url.split(&quot;/&quot;)[-1] dirs = Path(&quot;dataset/mnist&quot;) path = (dirs / name) if path.exists(): with path.open(&quot;rb&quot;) as f: dat = f.read() else: if not dirs.is_dir(): dirs.mkdir(parents=True, exist_ok=True) with path.open(&quot;wb&quot;) as f: dat = requests.get(url).content f.write(dat) return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy() def mnist_dataset(): X_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;)[8:] X_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;)[8:] return (X_train, Y_train, X_test, Y_test) X_train, Y_train, X_test, Y_test = mnist_dataset() class Net: def __init__(self, lr=0.001): # parameters self.w1 = self.init_uniform(784, 128) self.w2 = self.init_uniform(128, 10) self.lr = lr @staticmethod def init_uniform(a, b, dtype=np.float32): return np.random.uniform(-1., 1., size=(a, b)).astype(dtype) / np.sqrt(a*b) @staticmethod def relu(x): return np.maximum(x, 0) @staticmethod def linear(a, b): return a @ b @staticmethod def logsumexp(x): c = x.max(axis=1) return x - (c + np.log(np.exp(x-c.reshape((-1, 1))).sum(axis=1))).reshape((-1, 1)) def calc_loss(self): # crossentropyloss self.x_lsm = self.logsumexp(self.x_w2) self.loss = (-self.out * self.x_lsm).mean(axis=1).mean() def calc_optim(self): # Update (SGD) self.w1 += -self.lr * self.d_w1 self.w2 += -self.lr * self.d_w2 def forward(self, X): self.X = X self.x_w1 = self.linear(X, self.w1) self.x_relu = self.relu(self.x_w1) self.x_w2 = self.linear(self.x_relu, self.w2) return self.x_w2 def backward(self, Y): # target -&gt; one-hot encoded self.out = np.zeros((len(Y),10), np.float32) self.out[range(self.out.shape[0]), Y] = 1 # loss function self.calc_loss() # derivative of target d_out = -self.out / len(Y) # derivative of loss with respect to target dx_lsm = d_out - np.exp(self.x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 self.d_w2 = self.x_relu.T @ dx_lsm # derivative of ReLU d_relu = dx_lsm @ self.w2.T d_relu[self.x_w1 &lt; 0] = 0 # derivative of l1 self.d_w1 = self.X.T @ d_relu net = Net() batch_size = 128 losses, accuracies = [], [] ## Train for i in (t := trange(10000)): # Batch of training data &amp; target data samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] output = net.forward(X) net.backward(Y) net.calc_optim() # Save for statistic cat = np.argmax(output, axis=1) # results from Net acc = (cat == Y).mean() accuracies.append(acc) losses.append(net.loss) t.set_description(f&quot;Loss: {net.loss:.5f}; Acc: {acc:.5f}&quot;) plt.ylim(-0.01, 1.1) plt.plot(losses) plt.plot(accuracies) plt.legend([&quot;losses&quot;, &quot;accuracies&quot;]) plt.show() ## Evaluation Y_test_preds = np.argmax(net.forward(X_test.reshape((-1, 28*28))), axis=1) true_acc = (Y_test == Y_test_preds).mean() print(f&quot;Accuracy on testing set: {true_acc}&quot;) . . You can compare it with the code from the PyTorch example and see differences. . Try it (Binder) . If you want to try NN right now, I have prepared here widgets for uploading and passing images to NN. Those widgets won&#39;t show up here, so you need to click on the Binder button at the beginning of this blog post. It will generate a jupyter notebook of this blog post in your browser. Be sure to have all pip dependencies installed. Then run all relevant cells and go to this section and here you will see widgets for uploading and evaluating image (the image will be automatically rescaled to 28x28 and converted to grayscale): . from ipywidgets import VBox, FileUpload, Button, Output, Label from IPython.display import display from PIL import Image, ImageOps import io btn_upload = FileUpload() btn_run = Button(description=&quot;Classify&quot;) out_pl = Output() lbl_pred = Label() def on_click_classify(change): # Convert Image from bytes to PIL format and than to numpy array fn = io.BytesIO(btn_upload.data[-1]) img = Image.open(fn).resize((28, 28)) img_ = ImageOps.grayscale(img) arr = np.asarray(img_) # get values from NN and show them output = net.forward(arr.reshape((1, 28*28))) predict = np.argmax(output, axis=1) confid = softmax(output) # display out_pl.clear_output() with out_pl: display(img_) lbl_pred.value = f&quot;Prediction: {predict[0]}; Confidence: {confid[0][predict[0]]:.4f}&quot; btn_run.on_click(on_click_classify) VBox([Label(&quot;Select your digit! (will classify last uploaded)&quot;), btn_upload, btn_run, out_pl, lbl_pred]) . Conclusion . I hope it helped. We have made a neural network that classifies digits on a human level. Despite our neural network is specialized only for classification, I still think it is impressive to build something like this. . . Tip: It is impractical to write every time whole neural networks from scratch. Among all deep learning libraries, I recommend PyTorch. It is fundamentally the same as NumPy, but with GPU for computations, NN tools, and its code is written greatly. .",
            "url": "https://ludius0.github.io/my-blog/ai/deep%20learning%20(dl)/2020/12/14/Neural-network-from-scratch.html",
            "relUrl": "/ai/deep%20learning%20(dl)/2020/12/14/Neural-network-from-scratch.html",
            "date": " • Dec 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m ludius0. I have been programming almost year and I have been finishing my last high school year. I don’t have social media, but I have github account and youtube channel. . Also I generated this website with fastpages 1. . a blogging platform that natively supports Jupyter notebooks. It’s great check it out if you want also do blogging. &#8617; . |",
          "url": "https://ludius0.github.io/my-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ludius0.github.io/my-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}