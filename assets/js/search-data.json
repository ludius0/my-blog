{
  
    
        "post0": {
            "title": "Neural network from scratch (Still in progress of writing it)",
            "content": "To have genuine understanding of neural networks is valuable, because they feel like &quot;black boxes&quot;. To be sure we understand it, we should build one from scratch. In this post I will explain concepts behind it and build one from scratch for classifying MNIST dataset. . Prerequisities . Code is written in python 3.8+ and I use library called numpy for matrixes multiplication. A math behind neural networks can looks like daughting, but they are basically high school math. . This is everything I am going to import: . import numpy as np import matplotlib.pyplot as plt # for plotting arrays from tqdm import trange # for fancy loading console output . Numpy . If you are already familiar with the numpy feel free to skip this part. If you are not familiar at all with matrixies or arrays than rather check this source. Numpy is covered there far deeper. If you only need fast recap or you&#39;re somehow familiar with the array idea than continue. . For building neural network we need tensor. Those can be represented by numpy arrays, so what are numpy arrays? . They are basically same as python list class, but better. Because numpy library is also written in the C/C++ it can perform faster than python native list class. We can easely create matrix of shape (3, 3). . np.zeros((3, 3)) . array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) . Where with the native list in the python we need create loop in a loop and that make it sluggish and kind of hard to read. . matrix = [] for i in range(3): matrix.append([]) for j in range(3): matrix[-1].append(0) matrix . [[0, 0, 0], [0, 0, 0], [0, 0, 0]] . We can use list comprehensions . [[0 for j in range(3)] for i in range(3)] . [[0, 0, 0], [0, 0, 0], [0, 0, 0]] . but numpy is still faster and more cleaner and offer a lot of funciton we will use. Im going to explain only things we need to know from numpy. First thing is that numpy arrays share memory for efficiency . a = np.zeros((3, 3)) b = a b[1, 1] = 1 a . array([[0., 0., 0.], [0., 1., 0.], [0., 0., 0.]]) . When creating some array we have to pass shape we want to have it. . shape = (2, 4) arr = np.random.uniform(-1, 1, shape) # -1 to 1 is range arr . array([[ 0.57771293, 0.0799716 , 0.16431917, 0.24523494], [-0.47948191, 0.40615386, 0.80683109, 0.11687376]]) . We can also ask it for a shape . arr.shape . (2, 4) . We can also perform all basic operations like sum, log, exp, add and so on.... Those can be perform if it does not go against its shape. . np.exp(arr) + 1 - np.array([-1, -3, 2, 8]) # checkout their different shapes . array([[ 3.7819583 , 5.0832563 , 0.17859042, -5.72207849], [ 2.61910406, 5.50103348, 1.24079585, -5.87602247]]) . We also use special operation called np.dot, which multiply two arrays by taking rows of first array and columns of second array. If our array doesn&#39;t match needed shape we can flip it with .T operation. . arr1 = np.random.uniform(-1, 1, (2, 4)) arr2 = np.random.uniform(-1, 1, (2, 4)) np.dot(arr1.T, arr2) . array([[ 0.02648154, 0.01904228, -0.04536944, 0.0054643 ], [-0.06827555, -0.03120425, 0.0432983 , 0.02729615], [-0.87929611, -0.27277719, 0.02603468, 0.65014024], [ 0.05830295, 0.16100189, -0.59024111, 0.28747108]]) . Numpy arrays can be also used for plotting: . a = np.eye(3, 3) plt.imshow(a) plt.show() . MNIST dataset . Before we start to build one we have to import dataset to work with. For that I have prepared this code: . import requests, gzip, os, hashlib def fetch(url): filename = url.split(&quot;/&quot;)[-1] full_filename = &quot;dataset/mnist/&quot; + f&quot;{filename}&quot; if os.path.isfile(full_filename): with open(full_filename, &quot;rb&quot;) as f: dat = f.read() else: if not os.path.isdir(&quot;dataset&quot;): os.mkdir(&quot;dataset&quot;) if not os.path.isdir(&quot;dataset/mnist&quot;): os.mkdir(&quot;dataset/mnist&quot;) with open(full_filename, &quot;wb&quot;) as f: dat = requests.get(url).content f.write(dat) return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy() def mnist_dataset(): X_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_train = fetch(&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;)[8:] X_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;)[0x10:].reshape((-1, 28, 28)) Y_test = fetch(&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;)[8:] return (X_train, Y_train, X_test, Y_test) . MNIST dataset is made from 70000 images. It is split in two types: training (60k) &amp; testing (10k) data. Each image has own label, which describe it. . X_train, Y_train, X_test, Y_test = mnist_dataset() select = 1337 plt.imshow(X_train[select]) print(f&quot;Number {Y_train[select]}&quot;) . Number 6 . The reason we need testing data si for validating how (if even) our model function properly. Because how the neural network is created we need to transform this image to numpy 1D array (vector) and because our images have size of 28x28 pixels we will pass in our neural network 1D array of lenght 784. . select = 1337 plt.imshow(X_train[select].reshape(1, -1)) print(f&quot;Number {Y_train[select]}&quot;) . Number 6 . 28*28 = 784 . How it looks like? . To have some visual image of what we&#39;re going to build: . Credit: https://github.com/fastai/fastbook/blob/master/01_intro.ipynb . This is how Neural Network (NN) looks like! Our inputs are MNIST images in the form of numpy 2D array (matrix) and which contains numbers between 0 to 1. Usually we represent data between 0 to 1 or -1 to 1; It is unwritten golden rule. Architecture (model) is basically our neural network. There we pass inputs through nodes (neurons), which contain weights and those are represented as parameters and we get predictions. We than take labels, which are representing our targets and compare them with our prediction. We measure how wrong our model was with loss function and use it to update parameters so it could &quot;learn&quot;. . Maybe now is good question &quot;what is even neural network?&quot;. It is inspired from neurons in our brain. Neuron &quot;fire&quot; signal if it get certain amount of input and millions of them can create complex structures. Right now there are three types of structure in the field: linear, convolutional and reccurent. We use simplest and oldest one the linear structure: . outputs = weights * inputs + biases . As we can see it is linear function. In our model we won&#39;t use biases, so weights are only parameters we will be updating in training loop. With this our model will be learning on it&#39;s own and when our programming function is creating own function, we call it machine learning. So far so good, but to clarify, every node (neuron) will be linear function and we stack them as full layer and pass every input to every node in the layer (to imitate complex structures) each node will have own weight as parameter. We also stack layers and &quot;connect&quot; them together by weights. . This image roughly show how neural network looks like. Here it is defined with: input layer (16 nodes), hidden layer 1 (12 nodes), hidden layer 2 (12 nodes), output layer (1 node). . Created with: https://alexlenail.me/NN-SVG/index.html . Each of those &quot;strings&quot; are weights and we can see that every node has own weight for every node from previous layer. We will create only two layers: hidden layer 1 with 128 nodes and hidden layer 2 with 128 nodes. Our input layer is image (reshaped to 1D array), where each &quot;pixel&quot; is one node in the input layer. Last layer is output layer and because we classify 0-9 images it will have 10 node. . Pytorch example . To show you what everything we will be implementing from scratch I have prepared example made with popular framework pytorch. Just hit &quot;show code&quot; button to have some picture how code will looks like. . #hide-output import torch import torch.nn as nn torch.manual_seed(7) class TorchNet(nn.Module): def __init__(self): super(TorchNet, self).__init__() self.l1 = nn.Linear(784, 128, bias=False) self.l2 = nn.Linear(128, 10) self.sm = nn.LogSoftmax(dim=1) self.act = nn.ReLU() def forward(self, x): x = self.act(self.l1(x)) x = self.l2(x) x = self.sm(x) return x model = TorchNet() loss_function = nn.NLLLoss(reduction=&#39;none&#39;) optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0) BS = 128 losses, accuracies = [], [] for i in (t := trange(10000)): # Create Batch (randomly initiated) samp = np.random.randint(0, X_train.shape[0], size=(BS)) X = torch.tensor(X_train[samp].reshape((-1, 28*28))).float() Y = torch.tensor(Y_train[samp]).long() # Pass through Net (+ calculate loss) output = model.forward(X) loss = loss_function(output, Y).mean() # Update parameters optim.zero_grad() loss.backward() optim.step() # Save for statistic cat = torch.argmax(output, dim=1) acc = (cat == Y).float().mean() accuracies.append(acc.item()) losses.append(loss.item()) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) . . It evaluated about 96% on testing set. This everything we will be implementing from scratch. . Neural network structure . As the first image show in the How it looks like? we will be implementing everything what is there. The whole code for neural network should be under 100 lines of code in python. . Linear Layer (architecture / parameters) . Our input is goingt to have 784 lenght as 1D array (vector). That mean our weights need to have 784 rows and X columns. We&#39;re going to have only two hidden layers with a size of 128 so we need to initate matrix with a shape of (784, 128) and second with a shape of (128, 10). To create it we need to initiate some random numbers in those shapes. Note that first hidden layer already contain shape for input layer and second hidden layer contain shape for output layer . Every neural network frameworks use own initiate functions that create usually numbers between -1 to 1 or 0 to 1. I chose uniform function, because it performed best from what I have observed. . def _uniform(a, b, dtype=np.float32): return np.random.uniform(-1., 1., size=(a, b)).astype(dtype) / np.sqrt(a*b) . Now we can create our first layers . l1 = _uniform(784, 128) # layer one (weights) l2 = _uniform(128, 10) # layer two (weights) l1.shape . To perform our linear pass we take one image from training set and use np.dot. . def linear(a, b): return np.dot(a, b) img = X_train[0].reshape(-1) x_l1 = linear(img, l1) x_l1.shape . Full pass: . x_l1 = linear(img, l1) x_l2 = linear(x_l1, l2) x_l2.shape . Activation function (architecture) . What are they? They are non-linear function. We use them to have non-linear outputs. Imagine your input data are for example sinus. If our architecture would be only made by linear function it couldn&#39;t learn how to &quot;bend&quot; output to something similiar to sinus. . We use only one after first layer (hidden layer 1). Because we classify they are not neccesity, but we still get better performance. We implement ReLU and it is most popular one. ReLU is defined as if input &lt; 0 than 0 else input and to implement it is really easy: . def relu(x): return np.maximum(x, 0) . Implementation with our code: . That&#39;s it. We have implemented forward pass for our neural network so far. After we pass image we get output of size 10. But the real question is how it will learn? . Loss function . To teach neural network we have to figured how to represented how big is error (difference) between prediction and label. . We can represent label as one-hot encoded array; so if it is number 6 it create array (10,) of zeros and place one on its 6 position. . select = 1337 label = Y_train[select] input = X_train[select].reshape(1, -1) # Represent label as one-hot encoded array label_arr = np.zeros((10, ), np.float32) label_arr[label] = 1 # Forward pass x_l1 = np.dot(input, l1) x_relu = relu(x_l1) x_l2 = np.dot(x_relu, l2) # output output = np.argmax(x_l2) # Take biggest number in array (by position) # Differences print(f&quot;Correct number: {label_arr} -&gt; {label}&quot;) print(f&quot;Prediction: {x_l2} -&gt; {output}&quot;) . We can see its prediction is wrong. We can calculate error simply like label_arr - x_l2, but because our classes (targets/labels) are mutually exclusive we will use specialised function for classification called LogSoftmax. With that our differences are &quot;maginified&quot;. Here is implementation in a code: . def logsoftmax(x): return x_l2 - np.log(np.exp(x).sum(axis=1)).reshape((-1, 1)) x_lsm = logsoftmax(x_l2) x_lsm . We have our loss function, but we won&#39;t be implementing in forward pass. We use loss function to update our parameters, which are l1 and l2 so we are going to implement backward pass. . Backpropagation . The fundamental question is How do you meaningfully update parameters?. Answer is we use derivatives of functions we implemented in architecture. Derivatives are representing slope of original function. All those results from each layers give us multidimensional representation called gradiant descent. We calculate them to find out to which &quot;side&quot; we have to move to decrease loss and base on it we update parameters. This part is conceptually hardest one to understand. . Let&#39;s say we have only one simple layer with a loss function and we get this: . Our goal is to update parameters to decrease a loss, which represent &quot;how wrong model was&quot; so it would be next time more accurate. If we calculate derivative we can get something like in graph above (probably with more dimensions). Let&#39;s take that red dot as one of the weights and calculate derivative of linear function with respect to inputs. and we get something like this: . Now we know the slope, it just need some push to the right and for that we use use learning rate and optimizer later. . This concept can be hard to wrap head around. You can checkout all activation function and their derivatives here. Also checkout there sigmoid function and its derivative function to spot their differencies and remember that derivative function is a slope of original funciton. . We are interest only in the three types of derivatives function. Those are: Linear, ReLU and LogSoftmax. Also we will need fot LogSoftmax derivative of one-hot encoded label (target). . Also because we use loss function after forward in the model (architecture) we have to go backward to compute derivatives. . Fortunetly this section is fairly easy in the code, so here is whole backward pass with all derivatives of our functions: . x = X_train[1337].reshape(1, 784) y = np.array(Y_train[1337]).reshape(-1) #&lt;- target ## backward pass # target -&gt; one-hot encoded out = np.zeros((len(y), 10), np.float32) out[range(out.shape[0]), y] = 1 # logsoftmax loss function x_lsm = logsoftmax(x_l2) # Chain of derivates # derivative of Softmax to target d_out = -out / y # negative target (insure it won&#39;t fo crazy in float number) dx_lsm = d_out - np.exp(x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 to Softmax d_l2 = np.dot(x_relu.T, dx_lsm) dx_relu = np.dot(dx_lsm, l2.T) # derivative of ReLU to l2 dx_l1 = dx_relu.copy() # create a new copy that doesn&#39;t share memory dx_l1[x_l1 &lt; 0] = 0 # derivative of l1 to ReLU d_l1 = np.dot(x.T, dx_l1) . Note that for calculating derivatives we need to remember inputs from original functions. . There is only one last missing piece. We need to update our parameters and for that we use optimizer. . Optimizer . We aill use simple one called SGD (Stochastic gradient descent). Here we will need learning rate to say parameters &quot;how big step&quot; we want to do in the gradiant descent. . lr = 0.001 # SGD l1 += -lr * d_l1 l2 += -lr * d_l2 . That&#39;s all! We just need to construct training loop and also batches of data . Prepare data in a batches . So far we have been reshaping data with the shape (1, 784), but we will be computing multiple images at once for better results, that is called generalizing.We initiate batch randomly. I tried to make it orderly, but I got worse results. . batch_size = 128 samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] . Commonly is used batch sizes of 64, 128, 256. . Constructing training loop . When we get output from our model we threat it as one-hot encoded array and we tranform it to single number. We can than compare it with labels (targets) to keep track of accuracy. Also we will track of loss and to compute it we need sum it together. After the training loop we will plot losses and accuraccies. . output = x_l2 cat = np.argmax(output, axis=1) # from (10,) to (1) acc = (cat == label).mean() x_loss = (-out * x_lsm).mean(axis=1) loss = x_loss.mean() loss, acc, cat . Let&#39;s construct everything together: . l1 = _uniform(784, 128) l2 = _uniform(128, 10) lr = 0.001 batch_size = 128 losses, accuracies = [], [] ## Train for i in (t := trange(10000)): # Batch of training data &amp; target data samp = np.random.randint(0, X_train.shape[0], size=(batch_size)) X = X_train[samp].reshape((-1, 28*28)) Y = Y_train[samp] ## Forward pass x_l1 = linear(X, l1) x_relu = relu(x_l1) x_l2 = linear(x_relu, l2) output = x_l2 ## backward pass # target -&gt; one-hot encoded out = np.zeros((len(Y),10), np.float32) out[range(out.shape[0]), Y] = 1 # logsoftmax loss function x_lsm = logsoftmax(x_l2) # loss x_loss = (-out * x_lsm).mean(axis=1) loss = x_loss.mean() # Chain of derivates # derivative of Softmax to target d_out = -out / len(Y) # negative target (insure it won&#39;t fo crazy in float number) dx_lsm = d_out - np.exp(x_lsm) * d_out.sum(axis=1).reshape((-1, 1)) # derivative of l2 to Softmax d_l2 = np.dot(x_relu.T, dx_lsm) dx_relu = np.dot(dx_lsm, l2.T) # derivative of ReLU to l2 dx_l1 = dx_relu.copy() dx_l1[x_l1 &lt; 0] = 0 # derivative of l1 to ReLU d_l1 = np.dot(X.T, dx_l1) ## Update # SGD l1 += -lr * d_l1 l2 += -lr * d_l2 # Save for statistic cat = np.argmax(output, axis=1) # results from Net acc = (cat == Y).mean() accuracies.append(acc) losses.append(loss) t.set_description(f&quot;Loss: {loss:.5f}; Acc: {acc:.5f}&quot;) plt.ylim(-0.1, 1.1) plt.plot(losses) plt.plot(accuracies) plt.legend([&quot;losses&quot;, &quot;accuracies&quot;]) plt.show() . Loss: 0.00068; Acc: 1.00000: 100%|██████████| 10000/10000 [00:39&lt;00:00, 253.01it/s] . It looks good, but we need still to evaluate it. We do it by passing whole testing set. We don&#39;t need backward pass anymore if we don&#39;t plan to update it. . # Forward pass x_l1 = linear(X_test.reshape((-1, 28*28)), l1) x_relu = relu(x_l1) output = linear(x_relu, l2) # Measure accuracy Y_test_preds = np.argmax(output, axis=1) true_acc = (Y_test == Y_test_preds).mean() print(f&quot;Accuracy on testing set: {true_acc}&quot;) . Accuracy on testing set: 0.9801 . Conclusion . none yet... . Sources from which I was inspired to write this: . https://developer.ibm.com/technologies/artificial-intelligence/articles/neural-networks-from-scratch/ | https://www.youtube.com/watch?v=JRlyw6LO5qo | https://www.fast.ai | .",
            "url": "https://ludius0.github.io/my-blog/ai/deep%20learning/2020/12/05/First-blog-(test).html",
            "relUrl": "/ai/deep%20learning/2020/12/05/First-blog-(test).html",
            "date": " • Dec 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ludius0.github.io/my-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ludius0.github.io/my-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ludius0.github.io/my-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ludius0.github.io/my-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}